This is the html version of the file https://numenta.com/assets/pdf/whitepapers/hierarchical-temporal-memory-cortical-learning-algorithm-0.2.1-jp.pdf .
Google automatically generates html versions of documents as we crawl the web.

Page 1

HIERARCHICAL TEMPORAL MEMORY
including
HTM Cortical Learning Algorithms

VERSION 0.2, DECEMBER 10, 2010
Numenta, Inc. 2010

Use of Numenta’s software and intellectual property, including the ideas contained in this
document, are free for non-commercial research purposes. For details, see
http://www.numenta.com/software-overview/licensing.php

翻訳 株式会社アルトーク 2011/1/25
出典: http://www.numenta.com/htm-overview/education/HTM_CorticalLearningAlgorithms.pdf
脚注はすべて訳者による注釈である。本書の改訂版を確認するには http://ai.altalk.com 参照。 



Page 2

Numenta 翻訳ライセンス（参考和訳）

Copyright (c) 2010, 2011 Numenta, Inc.

All rights reserved.

ここに含まれる文章、アルゴリズム、サンプルコード、擬似コード、及びその他の記述は、
Numenta Inc.が発行した hierarchical temporal memory (“HTM”) 技術に関する記述の翻訳な
いしこれに基づいて得られたものである。原著の著作権及びここで翻訳された HTM やそのアル
ゴリズムに関する特許権は Numenta が保有している。独立した HTM システムの開発及び使用
に関して、それが商用目的ないし製品化目的ではなく、研究目的である限り、Numenta はその
特許権を主張しないことに同意する。Numenta の特許権に抵触する商用目的ないし製品化目的
のいかなる HTM 技術の使用も、Numenta から商用ライセンスを取得しなければならない。

上記に基づいて Numenta は貴方に、商用目的ないし製品化目的の使用ではなく、研究目的に限
り、これらのアルゴリズム及び著作を使用することを認可する。前述の「商用目的ないし製品化
目的の使用」は、訓練された HTM ネットワークないしアプリケーションを後に商用目的ないし
製品化目的で適用することを意図している場合、HTM ネットワークを訓練することを含む。前
述の「商用目的ないし製品化目的の使用」はまた、商用目的ないし製品化目的で HTM 技術の出
力結果を使用ないし他者に使用を許可することを含む。この記述を頒布・出版・複製したあらゆ
る記述には、この翻訳ライセンスの全文が英文及び翻訳対象言語の両方で含まれていなければな
らない。

このライセンスは明示的にも暗黙的にも特許権の使用を何ら許可しない。ここで許可された翻訳
物の品質ないし正確さに関して Numenta は義務も責任も負わない。 



Page 4

最初にお読 ry
本書は、この資料のドラフト ry 欠落している ry

本書に含まれること：
本書は Numenta が 2010 年に開発した学習と予測に関する新しいアルゴリズム ry
。 ry 実装可能なほど十分詳細 ry 。最初の章で概念 ry Numenta についてよく知っていて、
我々のこれまでの論文のいくつかを読んだことがあるなら、それらは馴染 ry

本書に含まれないこと：
ry 、この初期の草稿 ry

- ry 多くの側面は実装及びテストされているが、テスト結果 ry
- ry 実際の問題にどのように適用可能 ry 。 ry データを ry 分散表現に変換する方法 ry
- ry できる。オンライン学習 ry 特殊な状況下で必要となるいくつかの詳細 ry
- 執筆予定のその他の議論として次のものがある。疎分散表現の特徴に関する議論、
利用例・応用例、付録への引用。

我々は現時点で紹介可能な範囲で ry 。 ry
欠落 ry は、 ry 理解し実験をする上で妨げにならない ry 。 ry 随時この資料を改訂する。 



Page 5

目次

Numenta 翻訳ライセンス（参考和訳） .	.	.	.	. 2

序文 .	.	.	.	.	.	.	.	. 8
:
  ソフトウェアのリリース .	.	.	.	.	. 9
:
  Numenta 社について
  著者について .	.	.	.	.	.	. 10
:
第１章： HTM 概説 .	.	.	.	.	.	. 11
:
    階層構造 .	.	.	.	.	.	.	. 12
    リージョン .	.	.	.	.	.	. 15
    疎分散表現 .	.	.	.	.	.	. 16
    時間の役割 .	.	.	.	.	.	. 17
    学習 .	.	.	.	.	.	.	. 19
    推論 .	.	.	.	.	.	.	. 20
    予測 .	.	.	.	.	.	.	. 21
    行動 .	.	.	.	.	.	.	. 23
  HTM の実装に向けての進捗状況
第２章： HTM 大脳皮質性学習アルゴリズム .	.	.	. 24
:
    セル状態
    樹状突起セグメント .	.	.	.	.	. 25
    シナプス
  概要 .	.	.	.	.	.	.	. 26
  共通概念 .	.	.	.	.	.	.	. 32
    二値ウェイト
    永続値 .	.	.	.	.	.	.	. 33
    樹状突起セグメント
    シナプス候補
    学習



Page 6

  空間プーリングの概念 .	.	.	.	.	. 34
:
  時間プーリングの概念 .	.	.	.	.	. 36
:
  一次と可変長のシーケンスと予測 .	.	.	.	. 39
第３章： 空間プーリングの実装と疑似コード .	.	.	. 43
    初期化
    フェーズ 1: オーバラップ .	.	.	.	.	. 44
    フェーズ 2: 抑制
    フェーズ 3: 学習
  データ構造と補助関数 .	.	.	.	.	. 46
第４章： 時間プーリングの実装と疑似コード .	.	.	. 48
  時間プーリング疑似コード： 推論だけのバージョン
:
  時間プーリング疑似コード： 推論と学習を含むバージョン .	. 49
:
  実装の詳細と用語説明 .	.	.	.	.	. 52
付録 A: 生体ニューロンと HTM セルの比較 .	.	.	. 57
  生体ニューロン
    細胞体 .	.	.	.	.	.	.	. 58
    主要樹状突起
    末梢樹状突起
    シナプス .	.	.	.	.	.	.	. 59
    ニューロンの出力 .	.	.	.	.	.	. 60
  単純な人工ニューロン
  HTM セル .	.	.	.	.	.	.	. 61
    主要樹状突起
    末梢樹状突起 .	.	.	.	.	.	. 62
    シナプス .	.	.	.	.	.	.	. 63
    セル出力
:
付録 B: 新皮質の層と HTM リージョンの比較.	.	.	. 65
  新皮質の神経回路網



Page 7

    層 .	.	.	.	.	.	.	. 66
    リージョンの違いによる層のバリエーション .	.	.	. 67
    カラム .	.	.	.	.	.	.	. 68
    ミニカラム .	.	.	.	.	.	. 69
    カラム反応の例外 .	.	.	.	.	.	. 70
  なぜ層とカラムがあるのか？ .	.	.	.	.	. 71
  異なる層が何をするのかに関する仮説 .	.	.	.	. 72
    HTM リージョンは新皮質の何に相当するか？ .	.	.	. 75
:



Page 8

序文

人間には簡単にできて、コンピュータには今のところできないようなことがたくさ ry

Hierarchical Temporal Memory (HTM) は、新皮質がこの様な機能を発揮する様子を
モデル化する技術である。 HTM は人間と同等あるいはそれ以上の多くの認識性能 ry

ry １章は HTM ry 階層構造 ry 、疎分散表現1、時間的な変化に基づく学習 ry
２章は HTM 大脳皮質性学習アルゴリズム2 ry
３章と第 4 章は HTM 学習アルゴリズムの疑似コードを、
空間プーリング及び時間プーリングの２つのパート ry
２章から第４章を読めば、熟練したソフトウェア技術 ry 実装して実験 ry

1 sparse distributed representation。本書を理解する上で重要な概念だが、
冒頭で述べられているように、残念ながら本書では説明されていない。
理論的基礎は Pentti Kanerva 著 Sparse Distributed Memory に詳しい。
Kanerva 氏は Jeff Hawkins 氏が設立した Redwood Neuroscience Institute
（現在は Redwood Center for Theoretical Neuroscience)の研究員。
2 HTM cortical learning algorithms



Page 9

ソフトウェアのリリース

ry ソフトウェアリリースは 2011 年中頃を予定している。

以前の文書との関係

HTM 理論の一部は 2004 年の On Intelligence3、Numenta 社から発行された
ホワイトペーパー、Numenta の従業員が執筆した論文など ry
２章から第４章で述べている HTM 学習アルゴリズムは、これまで発表されたことはない。
この新しいアルゴリズムは、Zeta 1 と呼ばれていた我々の第一世代アルゴリズムを置き換 ry
新しいアルゴ ry を“Fixed-density Distributed Representations” ry “FDR” と呼んでいた ry
HTM 大脳皮質性学習アルゴリズム4、あるいは単に HTM 学習アルゴリズムと呼んでいる。

ry Numenta 社創設者の一人である Jeff Hawkins と Sandra Blakeslee ry On Intelligence ry
この本は HTM という名前で述べてはいないものの、
それは HTM 理論とその背景にあるニューロサイエンスについて、
読みやすくかつあまり技術よりにならずに説明している。 ry 執筆された当時、
我々は HTM の基本原理を理解していたが ry 実装する方法を知らなかった。 ry

Numenta 社 ry

Numenta, Inc. (www.numenta.com)は HTM 技術を商業的ないし学術的利用のため ry
進捗及び発見を完全に文書化 ry
ソフトウェアを他の人が研究目的や商業目的で利用できるように提供 ry
アプリケーション開発コミュニティが立ち上がることを支援 ry 研究目的で自由に利用

3 邦訳 「考える脳 考えるコンピューター」ランダムハウス講談社
4 HTM Cortical Learning Algorithms



Page 10

してよい。我々は商業目的での技術サポートの提供、ソフトウェアライセンス販売、
知的所有権のライセンス販売で収益を得ている。
我々は常に開発パートナーを求めており、彼らと我々自身の成功 ry
Numenta 社はカリフォルニア州 Redwood City に拠点をおき、自己資本 ry

著者 ry

本書は Numenta 社の従業員の協力と努力 ry 名前は改訂記録 ry

改訂記録

各版の主な変更点 ry 。細かな修正や整形などは ry

版	日付	変更点	主な著者

0.1 	2010/11/9
1. 序文、1,2,3,4 章、用語集の初版
Jeff Hawkins, Subutai Ahmad, Donna Dubinsky

0.1.1 	2010/11/23
1. 第 1 章のリージョンの節で、レベル、カラム、層などの用語を明確化 ry
2. 付録Ａの初版
Jeff Hawkins

0.2	2010/12/10
1. 第２章： 明確化のため修正
2. 第４章：行番号を修正、37行と39行のコードを修正
3. 付録Ｂの初版
Hawkins Ahmad Hawkins



Page 11

第１章： HTM 概説

Hierarchical Temporal Memory (HTM) は、新皮質の構造的・アルゴリズム的性質
を捉えることを目指した機械学習技術である。

ry 。新皮質は実に均一なパターンのニューラル回路 ry

ry 小さなサブセットを実装 ry 今後より一層多くの理論が実装されるだろう。
現在我々は、新皮質の商業的ないし科学的価値のうちの十分なサブセットを実装 ry

HTM のプログラミングは伝統的なコンピュータプログラミングとは異なる。 ry
HTM はセンサーから得られたデータの流れに触れることで訓練される。
HTM の能力はそれがどのようなデータに触れたかによって ry

ry 「ニューラルネ ry 」という用語は非常に多くのシステムで用い ry 不用意に使えない。
HTM がモデル化するニューロン（HTM ではセル5と呼ぶ）は、
カラム6、層7、リージョン8、階層構造9の中に配置 ry
HTM は基本的にメモリベース ry
。HTM ネットワークは時間的に変化するたくさんのデータによって訓練され、
多くのパターンとシーケンス10の蓄積に依存している。データを
格納及びアクセスする方法は、一般にプログラマが使用する標準的なモデルとは論理的に異 ry
。伝統的なコンピュータメモリはフラット ry 時間に関する概念を持たない。 ry

5 cell
6 column
7 layer
8 region
9 hierarchy
10 sequence。連続して起こる事柄、ないしその順序を意味する。



Page 12

ry 。HTM のメモリは階層的な構造であり、時刻の概念が内在 ry
。情報は常に分散型の様式で保存される。HTM の利用者は階層のサイズを指定し、
何に対してそのシステムを訓練するのかを決める。 ry

HTM ネットワークは ry 、階層構造、時間と疎分散表現（ ry ）の主要機能を包含する限り、
我々は汎用的なコンピュータを使ってモデル化することができる。
我々はいずれ、HTM ネットワークに特化したハードウェアを作ることになる ry

本書では我々は HTM の特徴と原理を、人の視覚・触覚・聴覚・言語・行動を例にして表す。
ry 。しかしながら、HTM の能力は汎用的であることを心に留 ry
。HTM は学習と予測の機械であり、様々な種類の問題に適用可能である。

HTM 原論

ry なぜ階層的な組織が重要なのか、HTM のリージョンはどのように構成されるか、
なぜデータを疎分散表現で格納するのか、なぜ時間ベースの情報がクリティカルであるのか。

階層構造
ry 。リージョンは HTMにおける記憶と予測の主要構成要素 ry
。通常、各 HTMリージョンは階層構造の 1 レベルを表す。
階層構造を上がるに伴って、常に集約11がある。
子リージョンの複数の要素が親リージョンの一つの要素に集約 ry
階層構造を下がるに伴って、フィードバック接続による情報の発散12がある。
（リージョンとレベルはほとんど同義 ry 。リージョンの内部的な機能 ry 「リージョン」 ry
、特に階層構造の中でのリージョンの役割を指すときに「レベル」 ry ）

11 convergence
12 divergence



Page 13

図 １-１ ４階層の階層構造に配置された４つの HTMリージョンを単純化して表した図。
情報は階層間及び階層内部で通信 ry

複数の HTM ネットワークを結合することもできる。 ry 。
例えば、一つのネットワークが音声情報を処理し、他 ry が映像情報を処理する場合 ry
。各個別のネットワークがトップに向かうにつれて集約 ry

図 １-２ 異なるセンサから集約するネットワーク



Page 14

階層 ry 効率 ry 各レベルで学習されたパターンが上位のレベルで組み合わせて再利用 ry
学習時間とメモリ消費を非常に節約する。
説明のため、視覚 ry 。階層構造の最下位レベルでは、
脳は縁13や角などの視覚のごく一部分に関する情報を格納する。
縁は ry 基本的な構成要素である。これらの下位レベルのパターンは中間レベルで ry
曲線や模様などのより複雑な構成要素に集約される。
円弧は耳の縁 ry 車のハンドル ry カップの取っ手 ry
。これらの中間レベルのパターンはさらに集約されて、頭、車、家などの
高レベルな物体の特徴を表す。
高レベルな物体 ry 、その構成要素を再度学習する必要がなくなる。

ry 単語を学習 ry 文字や文節、発音を再度学習する必要はない。

階層構造間で表現を共有 ry 、予期される行動の一般化にもなる。
ry 動物を見 ry 、口や歯を見 ry 食 ry 噛 ry 予測 ry
。階層構造により、 ry 新しい物体がその構成要素が持つ既に分かっている特徴を
引き継いでいることを知ることができる。

一つの HTM 階層構造はいくつの事柄を学習 ry ？
言い換えれば、階層構造にはいくつのレベルが必要 ry ？
各レベルに割り当てるメモリと、必要なレベル数の間にはトレードオフ ry
HTM は入力の統計及び割り当てられたリソースの量とから、
各レベルの最適な表現を自動的に学習 ry
多くのメモリを割り当 ry レベルはより大きくより複雑な表現を構成し、
従って必要となる階層構造のレベルはより少 ry
少ないメモリ ry 小さく単純な表現を構成し、 ry レベルはより多 ry

ここからは、視覚の推論14のような難しい問題について述べる
（推論はパターン認識と似 ry ）。しかし多くの価値ある問題は視覚より単純で、
一つの HTM リージョンでも十分 ry
Web ry どこをクリックするか予測 ry
。この問題は、一連の Web クリックのデータをHTM ネットワークに流し込 ry
。この問題では空間的階層構造はわずか ry 。解決策は主に時間的な統計 ry
一般的なユーザのパターンを認識することで、 ry どこをクリックするかを予測 ry

13 edge。へり・ふち。
14 inference



Page 15

まとめると、階層構造は学習時間 ry メモリ消費を節約し、一般化 ry
しかしながら、単純な予測問題の多くは一つの HTM リージョンでも解決 ry

リージョン
階層構造に連結されたリージョン15の表現は、生物学から ry
新皮質は厚さ 2mm ry 。生物学では主にそれらが互いにどのように接続 ry
基づいて、新皮質を異なる領域ないしリージョンに区分けする。
あるリージョンはセンサから直接入力 ry
、他のリージョンは他のいくつかのリージョンを経由 ry
。階層構造を決 ry ージョ ry ョンへの接続関係 ry

新皮質のすべてのリージョンの細部は似 ry
サイズや階層構造の中のどこに位置 ry 違 ry 、その他は似ている。
厚さ 2mm の新皮質リージョンを縦にスライスしたなら、6 つの層 ry 。
5 つはセルの層で、1 つはセルではない層である（少しの例外 ry ）。
新皮質リージョンの各層はカラム状に数多くの相互接続されたセルがある。

HTM リージョンもまた、高度に相互接続されたセルがカラム状に配列された皮 ry
新皮質の第 3 層はニューロンの主要なフィード・フォワード層である。
HTM リー ry のセルはおおまかに言えば新皮質のリー ry 3 層にあるニューロンと等価 ry

図 １-３ HTM リージョンの区画。 ry 。セルは二次元のカラム状 ry
図では、1 カラム当たり 4 つのセル ry 小さな区画 ry 。各カラムは入力

15 region。体の部位、局部など ry



Page 16

の一部に接続され、各セルは同一リージョン内の他のセルに接続する（ ry 図 ry ない）。
この HTM リージョン及びそのカラム構造は新皮質リージョンの一つの層に等価 ry

HTM リージョンは新皮質リージョンのほんの一部と等価であるに過ぎないものの、
複雑なデータ列の推論と予測 ry 多くの問題に有益 ry

疎分散表現
新皮質のニューロンは高度に相互接続しているが、わずかなパーセンテージのニューロン
だけが一度にアクティブになるように抑制ニューロンによって保護されている。
よって脳内の情報は ry わずかなパーセンテージのアクティブなニューロンによって
表されている。この様なコード化は「疎分散表現」 ry 。「疎」とは、わずかなパーセ ry
。一つのアクティブなニューロンは何らかの意味表現に関わっているが、
いくつかのニューロンの文脈の中で解釈されて初めて完全に意味 ry

ry HTM リージョンの記憶の仕組みは疎分散表現に依存 ry 。
ry 入力 ry 疎であるとは限らないので、HTM リージョンが最初に ry 疎分散表現に変換 ry

ry リージョンが 20,000 ビットの入力 ry
。入力ビットの中の”1”や”0”の割合は、時間と共に非常に頻繁に変化 ry
ry 、またあるときは 9,000 個のビットが”1”であったりする。
HTMリージョンはこの入力を 10,000 ビットの内部表現に変換して、
入力 ry の 2%にあたる 200 ビットが一度にアクティブになるようにする。
ry 入力が時間と共に変化するに従って、内部表現もまた変化するが、
10,000 ビットのうち約 200 ビットが常にアクティブになる。

リージョン内で表現可能なものの数よりも起こりうる入力パターンの数の方が
ずっと大きいから、この処理によって多くの情報が失 ry 、と思 ry
。しかしながら、どちらの数も途方もなく大きい。
ry どのようにして疎表現を作成 ry 後述する。 ry 情報のロスは ry 問題にならない。 



Page 17

図 １-４ HTM リージョンのセルが疎分散的にアクティブ ry

時間の役割
時間は、学習・推論・予測において極めて重要 ry

時間を用いなければ、我々は触覚や聴覚からほとんど何も推論できない。
ry 目が不自由だとして、誰かが貴方の手の上にりんごを置い ry
。りんごの上で指を動かせば、触覚から得られる情報が常に変化しているにも関わらず、
ry 貴方が持つ「りんご」という高レベルの認識 ― は変化しない。しかし ry
手や指先を動かしてはいけない ry レモンではなくりんごであると識別するのは非常に難 ry

同じことは聴覚 ry 。変化しない音はわずかな意味しか持たない。
「りんご」という言葉や、誰かがりんごを噛んだときの音などは、
時間と共に素早く順序的に変化する数十から数百の音階の列 ry

視覚は対照的に、混在したケースである。 ry 一瞬だけ ry でも識別可能 ry
必ずしも時間的な入力の変化 ry 。しかし ry 常時、目や頭や体を動かしており、
物体もまた周囲を動き回 ry 。素早 ry 視覚的変化の中から推論する我々の能力は、
視覚の統計的な特徴と長年の訓練によってもたら



Page 18

される特別なケース ry

学習について見てみよう。 ry HTM システムは訓練の間、時間的に変化する入力に触 ry
。視覚では静的な画像の推論がときには可能なものの、
物体 ry を学習するため ry 変化する様子 ry 犬が ry 走ってくる様子 ry
網膜に一連のパ ry 、数学的に言えばそれらのパターンはほとんど似ても似つかない。
脳はこれらの異なるパターンが同じ ry 順序的な変化を観察することによって知る。
時間はどの空間的なパターンが一緒に現れるかを教えてくれる「先生」である。

センサから得られる入力が変化するだけ ry 無関係な入力パターンが続けて現れても
混乱するだけ ry 。また、ry 、非人間的なセンサも ry 適用できる点にも注意 ry
。もし発電所の温度・振動・雑音のパターンを認識するように HTM を訓練 ry
これらのセンサの時間的な変化からもたらされるデータで訓練 ry

ry HTM ネットワークは多くのデータで訓練 ry 。HTM アルゴリズムの仕事は、
データの時系列の流れ ry どのパターンに続いて ry ーンが現れるかというモデルを構築 ry
。この時系列がいつ始まりいつ終わるのかがわからないので、この仕事は難しい。
同時に複数の時系列が重なりあって起こることもある。
学習は継続的に行われ、またノイズがある中で行われなければならない。

シーケンスの学習と認識は予測を形成する基準 ry ーンが他のどのパターンに続くかを
HTM が学習すれば、 ry 現在の入力とその直前の入力に対して
次にどのパターンが現れる可能性が高いかを予測 ry

HTM の 4 つの基本的な機能に戻ろう：学習・推論・予測・行動16である。
各 HTMリージョンは最初の 3 ry 、学習・推論・予測を実行 ry しかし ry 行動は異なる。
生物学によれば、多くの新皮質のリージョンが行動を形成 ry
しかし我々は、多くの興味深いアプリケーションにおいてこれは重要ではないと信

16 behavior



Page 19

じている。よって行動は現在の HTM の実装に含まれていない。 ry

学習
HTM リージョンはセンサから得られるデータのパターンとパターンのシーケンス
を見つけることで、その世界を学習する。リージョンはその入力が何を表しているのか
を「知って」はいない。それは純粋に統計的な世界でのみ機能する。
それは入力ビットの組み合わせのうち、頻繁に同時に起こる組み合わせを見ている。
ry これを空間的パターンと呼 ry パターンが時間と共にどのような順で現れるか ry
これを時間的パターンないしシーケンスと呼んでいる。

もしリージョンへの入力が建物の環境に関するセンサであるなら、リージョンは
建物の北側や南側において、ある温度と湿度の組み合わせがしばしば起こることを
見つけるだろう。そしてこれらの組み合わせが毎日移り変わる様子を学習 ry

もしリージョンへの入力があるお店の購入に関する情報であれば、
週末にある種の雑誌が購入されることや、
天候が寒いときはある種の価格帯のものが夕方頃に好まれることを見つける ry

一つの HTM リージョンは学習の能力が限定されている。
リージョンはそれがどれだけのメモリを利用可能で、それが受け取った入力が
どのくらい複雑であるかに応じて何を学習するかを自動的に調整する。
リージョンに割り当てられたメモリが削減 ry 学習する空間的パターンはより単純 ry
メモリが増加 ry 複雑になりうる。学習した空間的パターンが単純であれば、
複雑な画像を理解するにはリージョンの階層構造が必要となりうる。
我々はこの特徴を、人の視覚システムに見る ry
。網膜から情報を受け取る新皮質のリージョンは、
視覚的な小さな領域についてだけ、空間的なパターンを学習する。
階層構造のいくつかのレベルを経由した後にだけ、視覚の全体像を認識する。

生物的システムと同様に、HTM リージョン ry オンライン学習 ry
新しい入力を受け取るごとに継続的に学習する。学習した後の方が推論が改善されるが、
学習フェーズと推論フェーズとを分ける必要はない。 ry

ry HTM は学習し続けることもできるし、訓練フェーズの後に学習を無効化することも ry
、階層構造の下位レベルでは学習を無効化し、上位レベルでは学習を続けることもできる。
HTM がその周囲の世界の統計的構造を学習したら、多くの学習は階層構造のより上位 ry
。もし HTM が下位レベル ry 新しいパターンに触れたら、これらの新しいパター



Page 20

ンを学習するのにより長時間必要 ry 。既に知っている言語の新しい単語を学習するのは
比較的容易 ry 慣れない発音の外国語の新しい単語 ry 、まだ下位レベルの発音を知ら ry

単にパターンを見つけることは、価値の高い可能性を秘めている。
マーケットの変動、病状の変化、天候、工場の生産、送電系統のような複雑なシステムの
障害などの、高レベルなパターンを理解することはそれ自体に価値がある。
それでも空間的・時間的パターンを学習することは推論と予測に先立って必要となる。

推論
HTM が周囲の世界のパターンを学習すると、新しい入力について推論ができ ry
HTM が入力を受け取ると、以前に学習した空間的ないし時間的パターンと照合する。
新しい入力が以前に格納したシーケンスとうまく適合することが、
推論とパターンマッチングの本質である。

メロディをどうやって理解する ry 最初の音を聞いただけでは ry
次の音を聞 ry まだ十分では ry 普通は３，４，ないしそれ以上の音 ry 。
HTM リージョンの推論も似ている。
それは継続的に入力列を見て、以前学習したシーケンスと照合を試みる。
HTM リージョンはシーケンスの最初から ry が普通はもっと流動的で、 ry
メロディがどこから始まっても貴方が理解できることと似 ry
HTM リージョンは分散表現を用いるので、
リージョンがシーケンスを記憶ないし推論することは上記のメロディの例よりも複雑 ry

貴方がセンサから新しい入力 ry すぐに明確になるとまでは言えないものの、
慣れ親しんだパターンを ry 容易に見つける ry
例えば貴方は、例え老人であっても若い人 ry 、ほとんどの人が話す「朝食」という言葉
を理解できる。同じ人が同じ「朝食」という単語を百回発音しても、その音は二度と、
貴方の蝸牛殻17（音の受容体）を正確に同じように刺激することはないにも関わら ry

HTM リージョンも脳と同じ問題に直面する：入力は決して正確に繰り返されない。
さらに、ちょうど脳と同じように、HTM リージョンは推論や訓練の最中にも新しい入力 ry
。HTM リージョンが新しい入力に対処する一つの方法は、疎分散表現 ry
。疎分散表現の鍵となる特徴は、パターンの一部

17 cochleae。かぎゅうかく。耳の奥にある渦巻き状の感覚器官。



Page 21

分だけをマッチングするだけでほぼ確実にマッチ ry

予測
HTM の各リージョンはパターンのシーケンスを格納する。格納されたシーケンスを
現在の入力とマッチングすることで、次に到着すると思われる入力の予測をする。
HTM リージョンは実際には疎分散表現の間の変遷を記録する。
あるときはその変遷はメロディの中の音に見られるように線形のシーケンスであるが、
一般的な場合は将来入力される可能性があるものが同時に多数予測される。
HTM リージョンはときには長期間に及ぶ過去の文脈に基づいて異なる予測をする。ry 。

HTM の予測の鍵となる特徴のいくつかを以下に示す。

1) 予測は継続的 ry
貴方は特に意識していなくても継続的に予測 ry HTM も同じ ry
歌 ry 次の音を予測 ry 。階段 ry 足がいつ次の段に触れるかを予測 ry 。
HTM リージョンでは、予測と推論はほとんど同じ ry 。予測は分離された処理ではなく ry 統合 ry

2) 予測は階層構造のすべてのレベルのすべてのリージョンで起こる
ry 。リージョンはそれが既に学習したパターンについて予測 ry
。言語の例では、低レベルのリージョンでは次の音素を予測し、高レベル ry 単語や句 ry

3) 予測は文脈依存 ry
予測は過去に何が起こったか、そして現在何が起こっているかに基づいて行われる。
従って直前の文脈に基づいて、ある入力から異なった予測が行われることがある。
HTM リージョンは必要なだけのより多くの直前の文脈を用いて学習し、
短時間ないし長時間の両方の文脈を保持 ry 可変長記憶18 ry
例えば、暗唱 ry ゲティスバーグ演説19 ry 。次の単語を予

18 variable order memory
19 Gettysburg Address。「人民の人民による人民のための政治」のフレーズが有名。
”Four score and seven years ago our fathers brought forth on this continent, a new nation,
conceived in Liberty, and dedicated to the proposition that all men are created equal...” (以下略)



Page 22

測するには、現在の単語だけでは全く不十分 ry  ”and” の次に
“seven” が来ることもあれば、 ry 。ときにはほんの少しの文脈で予測 ry
“four score and” と分かれば次の “seven”を予測できる。
他の場合はフレーズが繰り返される場合 ry 、より長い文脈を使う必要がある。

4) 予測は安定化に ry
あるリージョンの出力はその予測である。HTM の特徴の一つは、
リージョンからの出力はより安定しているということである。
安定とはつまり、階層構造の上位に行くほどよりゆっくりと変化し、長く継続 ry
。リージョンは ry 可能なときは時間軸の複数ステップ先 ry 5 ステップ先 ry
。新しい入力が到着したとき、新たに予測されたステップは変化するが、
しかしそのうちの 4 つの既に予測されたステップは変化しない。
ry 、出力の一部だけが変化するので、出力は入力よりも安定化している。 ry 。
歌の名前のような高レベルの概念は、 ry 音のような低レベルの概念よりもゆっくり ry

5) 予測により、新しい入力が予期されたものか予期しないものかが分かる
各リージョンは ry 、予期せぬ事が起こったということを知ることができる。
HTM は次の入力として起こりうる ry 一度に多数予測 ry
次に何が起こるかを正確に予測できるわけではないが、 ry
どの予測にも一致しないとき、何か普通でないことが起こった ry

6) 予測はシステムをノイズにより強く ry
HTM ry 、推論をその予測に従う方向へ向かわせる ry
話し言葉 ry 、次にどんな音が、単語が ry 予測により、欠落したデータを埋め合わせ ry
あいまいな音 ry 予測していることに基づいてその音を解釈 ry 、ノイズ ry 推論 ry

HTM リージョンの、シーケンス記憶、推論、予測は緊密に統合 ry



Page 23

行動20
我々の行動は我々が感じることに影響を及ぼす。
目を動かすに従って、網膜は変化する入力を受け取る。
手や指を動かせば、触った感触が変化する様子が脳に届く。
我々の ry 動作は、我々が感じることを変化させる。
センサ入力と筋肉運動は緊密に絡み合っている。

数十年来の主要な見方では、新皮質の単一のリージョンである第 1 運動野が、
新皮質内で運動を指令 ry その後、新皮質内のほとんどないしすべてのリージョンは、
低レベルの感覚野でさえ、運動に関する出力を出していることが分かった。
すべての皮質性リージョンは感覚と運動機能とを統合 ry

運動指令を生成することは予測することと似 ry
現存のフレームワーク中の各 HTM リージョンに運動出力を加える ry 。 ry 。

HTM の実装に向けての進捗状況

ry HTM 大脳皮質性学習アルゴリズム ry 、基本的なアーキテクチャは健全 ry
。これに続く３つの章は現在のアルゴリズムの状況を述べる。

この理論の多くの構成要素はまだ実装されていない。注意21、
リージョン間のフィードバック、特定のタイミング、センサ入力と行動の統合など ry

20 behavior
21 attention



Page 24

第２章： HTM 大脳皮質性学習アルゴリズム

ry 。第３章と第４章は疑似コードを用いて学習アルゴリズムの実装方法 ry

用語説明

ry ニューロサイエンスの用語を使用する。セル22、シナプス23、
シナプス候補24、樹状突起セグメント25、カラム26などの用語 ry
。学習アルゴリズムは理論上の必要に応じてニューロサイエンスの細部に照らし ry
。しかし ry 性能の問題 ry 働きを理解したと感じたときには処理速度を向上させる別の方法 ry
 、生物学的な詳細に厳格にこだわるのではなく、同じ結果が得られさえすれば ry
ニューロサイエンスの用語に慣れ親しんだ読者 ry 読者の予期するものとしばしば違う ry
。付録 ry HTM 学習アルゴリズムとニューロ生物学的に等価 ry 相違点・類似点 ry

セル状態
ry セルは３つの出力状態 ry 。フィード・フォワード入力によりアクティブな状態、
横方向の入力によりアクティブな状態（これは予測を表す）、アクティブでない状態である。
最初の出力状態はニューロンのアクション状態による短時間のはげしい出力27に相当する。
２つ目の出力状態はもっとゆっくりとした、安定した出力28に相当

22 cell
23 synapse
24 potential synapse
25 dendrite segment
26 column
27 a short burst of action potentials in a neuron



Page 25

する。 ry これら２つのアクティブ状態以上の詳細なモデル化の必要性はない ry
個々のアクション状態の強さの程度、アクティビティの発生頻度を表すスカラー量などは
モデル化する必要性を見いだせない。分散表現の利用は、
セルのアクティビティの程度を表すスカラー量をモデル化することを凌駕 ry

樹状突起セグメント
HTM セルは比較的リアルな（ ry 複雑な）樹状突起モデルを持つ。各 HTM セルは理論的に
一つの主要樹状突起セグメント29と 10~20 個の末梢樹状突起セグメント30を持つ。
主要樹状突起セグメントはフィード・フォワード入力 ry
末梢樹状突起セグメントは周辺のセルからの横方向の入力 ry
抑制セルは同じカラム中の全てのセルが
類似のフィード・フォワード入力に対して応答するように強制する。
単純化のため、各セルごとの主要樹状突起セグメントを取り除き、
同じカラム中のすべてのセルで共有する一つの主要樹状突起セグメントで置き換えた。
空間プーリング関数（後述）はカラム単位で、共有の樹状突起セグメントに対して作用する。
時間プーリング関数はカラム中の個々のセル単位で、末梢樹状突起セグメントに対 ry
。生物学的にはカラムに接続するような樹状突起セグメントは存在しない ry
この単純化は同等 ry

シナプス
ry 二値のウェイトを持つ。生物学上のシナプスは可変値のウェイト ry 確率的・不安定 ry
生体ニューロンはシナプスのウェイト値の正確な値に依存しているはずがない ry
。HTM の分散表現及び我々の樹状突起の計算モデルを利用すれば、
HTM シナプスに二値のウェイトを割り当てても何ら悪影響はないはずである。
シナプスの形成及び切断をモデル化 ry ２つの追加 ry ニューロサイエンスから援用した。
一つ目は、シナプス候補の概念である。これは樹状突起セグメントに十分近い位置を通る
すべての軸索を表し、シナプスを形成する可能性があるものである。
二つ目は、永続値である。これは各シナプス候補に割り当てられたスカラー値である。
ry 軸索と樹状突起の間の接続の度合 ry 度合は生物学的には、完全に分離した状態から、
接続はしていないがシナプスを形成し始めた状態、最小限にシナプスが接続した状態、
大きく完全にシナプスが接続された状態に至るまでの範囲を取る。シナプスの

28 a slower, steady rate of action potentials in a neuron
29 proximal dendrite segment。樹状突起のうち、ニューロンの中心部に近い部分。
30 distal dendrite segment。樹状突起のうち、末端に近い部分。
distalは末梢（まっしょう）・末端の意味。ちなみに末梢神経は peripheral nerve という。



Page 26

永続値は 0.0 から 1.0 まで ry 。学習にはシナプスの永続値の増加や減少が含まれる。
シナプスの永続値がしきい値を超えたら、ウェイト値 1 で接続されたことを表す。
しきい値より下回っていたら、ウェイト値 0 で切断 ry

概要

仮に読者が HTM リージョンだ ry 。貴方の入力は数千ないし数万のビット ry
入力ビットはセンサ入力データや、階層構造の下位の他のリージョンから来たデータである。
それらは複雑にオン・オフしている。これらの入力に対して貴方は何が出来るか？

我々はその答えを単純な形態で既に説明した。
各 HTM リージョンはその入力の共通のパターンを探し、 ry シーケンスを学習する。
シーケンスの記憶から、各リージョンは予測 ry
もう少し説明 ry 以下の３ステップ ry

1) 入力の疎分散表現を作成する
2) 以前の入力の文脈に基づいて、入力の表現を作成する
3) 以前の入力の文脈に基づいて、現在の入力からの予測をする

ry 詳細 ry

1) 入力の疎分散表現を作成する
リージョンへの入力を想像するには、それを巨大なビット列と考えるとよい。
脳内ではこれらはニューロンからの軸索にあたる。任意の時点で、
これらの入力のある部分はアクティブ（値１）、他の部分は非アクティブ（値０）である。
アクティブな入力ビットの比率は変化する。例えば0%から60%としよう。

HTMリージョンで行う最初の事は、この入力を疎な新しい表現に変換することである。
例えば、入力のうち40%がオンかも知れないが、新しい表現では2%だけがオンになる。
HTMリージョンは論理的にはカラムの集合からなる。各カラムは1又はそれ以上のセルから成る。
カラムは論理的には2Dの配列状に配置できるが、これは要件ではない。
リージョンの各カラムは入力ビットのユニークな部分集合
（普通は他のカラムと重なるが、完全に同じ部分集合になることはない）に接続される。
結果として、異なる入力パターンからは、レベル全体では異なるカラムのアクティベーション
を得る。最も強いアクティベーションを得たカラムは、
弱いアクティベーションを得たカラムを抑制、ないし非アクティブ化する。
（抑制は非常に局所的範囲からリージョン全体までの範囲で変化する円の円内で起こる）入力



Page 27

の疎表現は、抑制の後でどのカラムがアクティブでどれが非ア ry
。例え入力ビットのうちアクティブなビットの数が大幅に変化した場合であっても、
相対的に一定の割合のカラムがアクティブになるように抑制関数が定義される。

図 ２-１ カラムとセルからなる HTM リージョンの例。リージョンの一部分のみ ry
。各カラムは入力のユニークな部分集合によるアクティベーションを受け取る。
最も強いアクティベーションを受けたカラムが他の弱いアクティベーションを抑制する。
結果は入力の疎分散表現である。（アクティブなカラムは灰色で示した）

いま、入力パターンが変化 ry ほんの少しの入力ビットが変化したなら、
いくつかのカラムでは少し多く又は少し少ない入力ビットがオン状態になるが、
アクティブなカラムの集合はあまり大幅に変化しないだろう。
よって似た入力パターン（アクティブなビットの共通部分が非常に多いもの）からは
アクティブなカラムの比較的安定した集合に対応付けられる。
コード化がどのくらい安定 ry 、各カラムがどの入力に接続しているかに大きく依存 ry
この接続関係は、後述する方法で学習する。
これらのすべてのステップ（入力の部分集合から各カラムへの接続関係を学習し、
各カラムへの入力レベルを決定し、アクティブなカラムの疎な集合をえらぶために抑制 ry ）
を空間プーリングと呼ぶ。この用語は空間的に類似（アクティブなビットの共通部分が多い）
のパターンがプールされる（それらが共通の表現に互いにグループ化される）ことを意味する。 



Page 28

2) 以前の入力の文脈に基づいて、入力の表現を作成する
リージョンで行われる次の機能は、入力をカラムで表現したものを、
過去からの状態ないし文脈を含む新しい表現に変換 ry
。新しい表現は各カラムの一部のセル、普通は１カラムにつき１つのセルを
アクティブにすることで得られる（図 ２-２）。

「I ate a pear」と「I have eight pears」31の二つの話し言葉を聞く場合 ry
「ate」と「eight」は ry 発音が同じ ry 同じ反応をするニューロンが脳内のどこか ry
また一方、 ry 異なる文脈にあると反応するニューロンが脳内の他のどこか ry
。「ate」という音に対する表現は「I ate」 ry 「I have eight」 ry 異なる ry 。
「I ate a pear」と「I have eight pears」の二つの文を記憶したと想像してみよう。
「I ate…」 ry 「I have eight…」 ry 異なる予測 ry よって ry 異なる内部表現 ry

ある入力を異なる文脈では異なるコード変換をするというこの原理は、
認知とふるまいの普遍的な特徴であり、HTM リージョンの最も重要な機能の一つ ry

HTM リージョンの各カラムは複数のセルからなっている。
同じカラムのすべてのセルは同じフィード・フォワード入力を受け取る。
ry 。アクティブな各カラムごとに、どのセルをアクティブ ry 選択するかによって、
完全に同じ入力に対して異なる文脈では異なる表現をすることができる。
例 ry 。各カラムは 4 つのセルからなり、各入力は 100 個のアクティブなカラムで表現 ry
。カラムの中で一つのセルだけが一度にアクティブであるとすると、
完全に同じ入力に対して 4100 通り ry
。同じ入力は常に同じ組み合わせの 100 個のカラムがアクティブになるが、
文脈が異なればカラム中の異なるセルがアクティブになる。
これで同じ入力に対して非常に大きな数の文脈を表現可能となったが、
これらの異なる表現はどのくらいユニーク ry ？
4100 個の可能なパターンのうちからランダムに選択した 2 個は、
ほとんどの場合、約 25 個のセルが重複 ry
よって同じ入力を異なる文脈で表した 2 つの表現は、
約 25 個のセルが共通で 75 個のセルが異なっており、容易に区別 ry

HTM リージョンの一般的な規則 ry カラムがアクティブ ry
、そのカラム中のすべてのセルを見る。
もしそのカラム中の一つ又はそれ以上のセルが既に予測状態であれば、
それらのセルだけがアクティブになる。もしそのカラム

31 「私は梨を食べる」と「私は 8 個の梨を持っている」



Page 29

中のすべてのセルが予測状態でないならば、すべてのセルがアクティブになる。
ry ：ある入力パターンが期待されるなら、システムは予測状態のセルだけをアクティブ
にすることで期待通りであることを確認する。その入力パターンが期待と違うなら、
システムはカラム中のすべてのセルをアクティブにすることで、
「予期しない入力が発生したのであらゆる解釈が有りうる」ということを表す。

もし以前の状態が何もないなら、従って文脈も予測もないなら、
カラムがアクティブになるときは各カラム内のすべてのセルがアクティブになる。
ry 歌の最初の音を聞いたときと似ている。
文脈がなければ、 ry 予測できない：すべての選択肢が有効である。
以前の状態があるが入力が予期したものと合致しないときは、
アクティブなカラムのすべてのセルがアクティブ ry 。この決定はカラムごとに
行われるので、予測 ry は「オール・オア・ナッシング」32ではない。

図 ２-２ カラムの一部のセルをアクティブにすることで、
HTM リージョンは同じ入力の多くの異なる文脈を表現 ry
。カラムは予測状態のセルだけをアクティブにする。
予測状態のセルがないカラムでは、カラム中のすべてのセルをアク ry
。図は、あるカラムでは一つのセルだけがアク ry 、あるカラムではすべてのセルがアク ry

ry HTM セルは３つの状態を取る。セルがフィード・フォワード入力によってアク ry
単に「アク ry 」の用語 ry

32 all-or-nothing



Page 30

。セルが他の周囲のセルとの横方向の接続によってアクティブ ry 「予測状態」と呼ぶ（図 ２-３）。

3) 以前の入力の文脈に基づいて、現在の入力からの予測をするリージョンの
最後のステップは次に起こると考えられることを予測 ry
予測はステップ 2)で作成した、すべての以前の入力からの文脈を含む表現に基づ ry

リージョンが予測をするときは、将来のフィード・フォワード入力によって
アクティブになると考えられるすべてのセルをアクティブ（予測状態）にする。
リージョンの表現は疎であるので、同時に複数の予測がなされ得る。
例えばカラムのうちの 2%が入力によってアクティブになるとすると、
カラムの 20%が予測状態のセルとなることで10 個の異なる予測がなされ得る。
ry 40% ry 20 個 ry 。各カラムが 4 個のセルからなり、一度に一つだけがアクティブ
になるとすれば、セル全体の 10%が予測状態 ry

今後、疎分散表現の章が追加されれば、異なる予測が混じり合っても、
リージョンは特定の入力が予測されたのかそうでないのかを高い確信 ry

リージョンはどうやって予測 ry ？ 入力パターンが時間と共に変化するとき、
カラムとセルの異なる組み合わせが順次アクティブになる。
あるセルがアクティブになると、周囲のセルのうちすぐ直前にアクティブだったセルの
部分集合への接続を形成する。これらの接続は、そのアプリ ry で必要とされる学習速度
に応じて早く形成されたりゆっくりと形成されたりするように調整できる。
その後、すべてのセルはこれらの接続を見て、
どのセルが同時にアクティブになるかを探さなくてはならない。
もし接続がアクティブになったら、
セルはそれ自身が間もなくアクティブになることを予測することができ、予測状態に入る。
よってある組み合わせのセルがフィード・フォワード入力によってアクティブになると、
ひき続いて起こると考えられる他の組み合わせのセルが予測状態になる。 ry
歌を聞いていて次の音を予測 ry 瞬間と同様 ry 



Page 31

図 ２-３  ry リージョンのいくつかのセルがフィード・フォワード入力によってアク ry
（薄い灰色で示した）。他のあるセルは、アクティブなセルからの横方向の入力を
受け取って予測状態になる（濃い灰色で示した）。

まとめると、新しい入力が到着すると、アクティブなカラムの疎な部分集合が選択される。
各カラムの一つ又はそれ以上のセルがアクティブになり、これはまた同じリージョン内
のセル間の接続の学習内容に応じて他のセルを予測状態にする。
リージョン内の接続によってアクティブになったセルは、次に何が起こると考えられるか
についての予測を表す。次のフィード・フォワード入力が到着すると、
他のアクティブなカラムの疎な組み合わせが選択される。
新たにアクティブになったカラムが予期したものでないとき、
つまりどのセルもそれがアクティブになることを予測しなかったとき、
カラム中のすべてのセルをアク ry
。新たにアクティブになったカラムが一つ又はそれ以上の予測状態のセルを持つなら、
それらのセルだけがアクティブになる。リージョンの出力はリージョン内のすべてのセル
のアクティブ状態であり、フィード・フォワード入力によってアクティブになったセルと、
予測状態のためアクティブになったセルとからなる。

ry 、予測は次の時刻ステップだけではない。
HTM リージョンの予測は将来のいくつかのステップに及ぶことも ry
ry メロディの次の音を予測するだけではなく、例えば次の 4 つの音 ry 。 ry
リージョンの出力（リージョン内のアクティブ状態のセルと予測状態のセルの和集合）
は入力よりもゆっくりと変化する。
リージョンがメロディの次の 4 つの音を予測 ry 。メロディを文字 A, B, C, D, E, F, G
のシーケンスで表現する。最初の 2 音を聞いた後、リージョン



Page 32

はシーケンスを理解し、予測をし始める。それは C, D, E, F を予測する。
B のセル33は既にアクティブであるから B, C, D, E, F がそれぞれ 2 つのアクティブな状態
のどちらかになる。ry 次の音 C を聞く。アクティブ状態のセルと予測状態のセルの集合は
C, D, E, F, G を表す。入力パターンは B から C へとまったく違うものに変化したが、
20%のセルだけが変化 ry

HTM リージョンの出力はリージョン内のすべてのセルのアクティブ状態を示すベクトル
で表されるので、この例の出力では入力に比べて 5 倍安定 ry
階層構造に配置されたリージョンでは、階層構造を上に登るに従って時間的な安定性が増加 ry
表現に文脈を追加して予測を行う 2 つのステップを「時間プーリング」 ry
。パターンのシーケンスに対してゆっくりと変化する出力を生成することで、
時間と共に順に現れる異なるパターンを「プールする」34。
ry 別のレベルで詳細化 ry 空間プーリングと時間プーリングで共通の概念から始 ry

共通概念

空間 ry と時間プーリングの学習は似ている。どちらの場合の学習も、セル間の接続関係、
あるいはシナプスの形成を含む。時間プーリングは同じリージョン内のセル間の接続を学習する。
空間プーリングは入力ビットとカラムとのフィード・フォワード接続を学習する。

二値ウェイト
HTM のシナプスは 0 又は 1 の効果だけを持つ。多くの他のニューラルネットワークモデルでは
0 から 1 の範囲で変化するスカラー値のウェイトを用いるのと異な ry

33 英文では cells と複数形なので、B を表すセルは一つではないことが分かる。
A, B, C,...のそれぞれに対応する疎分散表現は、
リージョン全体の 2%のセルの組み合わせで表される。
34 何かを貯めこんで蓄積するというニュアンスから、
時間に関する情報を蓄積するものという意味と思われる。



Page 33

永続値
シナプスは学習を通じて継続的に形成されあるいは切断 ry
、各シナプスに（0.0 から 1.0 の）スカラー値を割り当て、接続がどのくらい永続的 ry
。接続が強化されれば、永続値は増加する。他の状況では、永続値は減少する。
ry しきい値（例えば 0.2）を上回れば、シナプスは形成され ry 下回れば、シナプスは無効 ry

樹状突起セグメント
シナプスは樹状突起セグメントに接続される。樹状突起には主要と末梢の 2 種 ry

- 主要樹状突起セグメントはフィード・フォワード入力との間のシナプスを形成する。
このタイプのセグメントのアクティブなシナプスは線形に加算され、これにより
カラムがフィード・フォワード入力によるアクティブ状態になるか否かが決定される。
- 末梢樹状突起セグメントは同じリージョン内のセル間のシナプスを形成する。
各セルはいくつかの末梢樹状突起セグメントを持つ。
ある末梢樹状突起セグメント上のアクティブなシナプスの合計がしきい値を超えたら、
接続されたセルは予測状態によりアクティブになる。
一つのセルに複数の末梢樹状突起セグメントがあるので、
セルの予測状態はそれぞれをしきい値で判定した結果の論理和 ry

シナプス候補
ry 、樹状突起セグメントはシナプス候補のリストを持つ。
すべてのシナプス候補は永続値 ry がしきい値を超えたら有効に機能するシナプスとなる。

学習
学習では樹状突起セグメント上のシナプス候補の永続値を増加・減少させる。
ry 用いられる規則は「ヘブの学習規則」35に似ている。
例えば、ある樹状突起セグメントがしきい値以上の入力を受け取ったために
セルがアクティブに ry 、そのセグメント上のシナプスの永続値を修正する。
シナプスがアクティブであり、従ってセルがアクティブになることに貢献した場合、
その永続値を増 ry 。ry がアクティブではなく ry 貢献しなかった場合、その

35 Hebbian learning rules。「細胞 A の軸索が細胞 B を発火させるのに十分近くにあり、
繰り返しあるいは絶え間なくその発火に参加するとき、
いくつかの成長過程あるいは代謝変化が一方あるいは両方の細胞に起こり、
細胞 B を発火させる細胞の 1 つとして細胞 A の効率が増加する。」



Page 34

永続値を減 ry 永続値を更新する正確な条件は、空間プーリングと時間プ ry とでは異な ry

空間プーリングの概念

空間プーリングの最も基本的な機能はリージョンへの入力を疎なパターンに変換 ry
。シーケンスを学習して予測 ry 疎分散パターンから始めることが必要 ry
。空間プーリング ry いくつかの到達目標がある。

1) すべてのカラムを使用する
HTM リージョンは入力の共通したパターンに対する表現を学習するための
固定数のカラムがある。一つの目的は、全体の ry すべてのカラムが確かに、 ry 学習 ry
。決してアクティブにならないようなカラムは必要でない。そうならないために、
各カラムがその周囲のカラムと相対的にどのくらい頻繁にアクティブになるかを常に監視 ry
アクティブになる相対的な頻度が低すぎるときは、そのカラムが勝者となるカラムの集合に
含まれ始めるようになるまで、その入力がアクティブになる基準をブースト36する。
本質的に、すべてのカラムは周囲のカラムと互いに競合しており、入力パターンに対する
表現に加わろうとしている。あるカラムがほとんどアクティブにならないときは、
そのカラムはもっと積極的になる。そうなると、他のカラムはその入力を変更させられて
少しだけ異なる入力パターンを表現し始める。

2) 望ましい密度を維持する
リージョンは入力に対する疎な表現を形成する必要がある。
最大の入力を受け取ったカラムは周囲のカラムを抑制する。
抑制範囲を決める半径は、そのカラムの受容野37のサイズに比例する
（従ってまた、小さなサイズからリージョン全体に至る範囲を取る）。
抑制半径の範囲内では、多くのアクティブな入力を受け取ったわずかなパーセンテージ
のカラムだけを「勝者」とする。その他のカラムは無効化される。
（抑制「半径」の語感は二次元状に配置されたカラムを暗示しているが、
この概念は他のトポロジにも適用できる）

36 boost。後押しする、増強するなどの意。後述のアルゴリズムで
ブースト値という変数が出現するため、そのままブーストと訳した。
37 receptive field



Page 35

3) 些細なパターンを避ける
すべてのカラムが些細ではない入力パターンを表すことが望まれる。この到達目標は、
カラムがアクティブになるために必要な最小のしきい値を設定することで達成できる。
例えば、しきい値を 50 とすると、カラムがアクティブになるには
その樹状突起セグメント上のアクティブなシナプスが 50 個以上必要であり、
ry あるレベル以上に複雑なパターンだけが表現されることが保証される。

4) 余分な接続関係を避ける
よく注意しないと、あるカラムが巨大な数の有効なシナプスを保持することが起こりうる。
すると、あまり関連性のない多くの異なる入力パターンに強く反応するようになる。
シナプスの異なる部分集合は異なるパターンに反応するだろう。
この問題を避けるため、勝者カラムに現在貢献していないシナプスすべて ry 永続値を減 ry
。貢献していないシナプスに確実に十分なペナルティを与えることで、
一つのカラムが表現する入力パターンが限定 ry

5) 自己調整的な受容野
実物の脳は高い可塑性38を示す。
新皮質のリージョンは、様々な変化に反応してまったく異なる事柄の表現を学習できる。
もし新皮質の一部が損傷したら、
損傷した部分が表現していたものを他の部分によって表現するように調整される。
もし感覚器官が損傷したり変化したりすると、
それに関連付けられていた部分の新皮質は何か他のことを表現するように調整される。
システムは自己調整的である。我々の HTM リージョンにも同様の柔軟性を求めたい。
あるリージョンに 10,000 個のカラムを割り当てたら、
入力を 10,000 個のカラムで最適に表現する方法を学習するべき ry
入力の統計的性質が変化したら、カラムは ry 最適に表現するように変化するべき ry
まとめると、HTM の設計者はリージョンに任意のリソースを割り当てることができて、
そのリージョンは利用可能なカラムと入力の統計的性質に基づいて入力を最適に表現 ry
できるべきである。
一般的な規則は、リージョンのカラムがより多くあれば、
各カラムは入力のより大きくより詳細なパターンを表現 ry
。なお一定の粗さを保つが39、カラムは普通、より稀にアクティブになる。

38 plastic。かそせい。物理的な可塑性とは固体に外力を加えて変形させ、
力を取り去ってももとに戻らない性質のこと。
脳の可塑性とは経験に応じて神経回路の組み換えや再構成を行う能力のこと。柔軟性、適応性。
39 粗さ(sparsity)はアクティブになるカラムの割合。カラムの数が増えても粗さは一定ということ。



Page 36

これら ry 到達目標を達成するために、新しい学習規則は必要ない。
アクティブでないカラムをブーストし、粗さを一定に保つために周囲のカラムを抑制し、
入力に対するしきい値の最小値を設け、多くのシナプス候補を蓄積・維持し、
その貢献度に応じてシナプスを追加・削除することで、
全体効果としてカラムは望ましい効果を達成するように動的に設定される。

空間プーリングの詳細

ry

1) 固定の数のビットからなる入力から始める。これらのビットはセンサからのデータ
であったり、階層構造の下位の他のリージョンからであったりする。

2) この入力を受取る固定の数のカラムをリージョンに割り当てる。
各カラムはそれに連結された樹状突起セグメントを持つ。
各樹状突起セグメントは入力ビットの部分集合を表すシナプス候補の集合を持つ。
各シナプス候補は永続値を持つ。
その永続値に基づいて、いくつかのシナプス候補が有効になる。

3) 与えられた任意の入力について、
アクティブな入力ビットと接続している有効なシナプスの数を各カラムごとに求める。

4) アクティブなシナプスの数にブースト値40を乗じる。ブースト値は、そのカラムが
周囲のものに比べてどのくらい頻繁にアクティブになったかに基づいて動的に決 ry

5) ブースト後に最大のアクティベーションを得たカラムは、
抑制半径内の固定のパーセンテージのカラム以外のものを無効化する。
抑制半径はそれ自体、入力ビットの広がり具合（又はファン・アウト）から動的に決 ry
。これでアクティブなカラムの疎な集合が得られた。

6) アクティブなカラムのそれぞれについて、すべてのシナプス候補の永続値を調節する。
アクティブな入力に割り当てられたシナプスの永続値は増加させる。
非アクティブな入力に割り当てられたシナプスの永続値は減少させる。
永続値の変更により、いくつかのシナプスが有効になったり無効 ry 。

時間プーリングの概念


40 boosting factor



Page 37

時間プーリングがシーケンスを学習し、予測をすることを思い出して欲しい。
基本的な方法は、あるセルがアクティブになったら直前にアクティブであった他のセルとの
接続を形成することである。これによりセルは、そのセルの接続を調べることで
いつそれがアクティブになるかを予測 ry 。すべてのセルがこれを行えば、
全体としてそれらはシーケンスを記憶してそれを思い出し ry 予測できる。
パターンのシーケンスを記憶するための集中記憶装置はなく、その代わりに
記憶は各セルに分散配置される。記憶が分散 ry 、システムはノイズや誤りに強くなる。 ry
、疎分散表現の重要な特徴を 2, 3 述べておくことは価値がある。

仮想的に、あるリージョンが全部で 10,000 個あるセルのうち、常に 200 個のセル
がアクティブになることで表現を形成しているとしよう（ ry 2%のセルがアクティブ）。
200 個のアクティブなセルで表される特定のパターンを記憶・理解するには ry
。これを行う単純な方法は、関心がある 200 個のアクティブなセルのリストを作成 ry
。ちょうど同じ 200 個のセルが再びアクティブになったことが分かれば、
そのパターンを理解したことになる。
しかしながら、200個のアクティブなセルのうち 20 個だけのリストを作成して、
残りの 180 個を無視したとしたら ry ？ 20 個のセルだけを記憶したら、
200個のセルの異なるパターンにおいてそれら 20 個の部分がちょうどアクティブに
なるようなパターンが数多く存在して、間違いだらけに ry はならない。
パターンは大きくかつ疎であるため（ry 10,000 個のうち 200 個のセルがアクティブ）、
20 個のアクティブなセルを記憶することで 200 個すべてを記憶することと
ほとんど同じくらいうまく記憶できる。
実際のシステムで間違いが起こる可能性は極めて稀 ry メモリ量を非常に節約 ry

HTM リージョンのセルはこの特徴を利用している。各セルの樹状突起セグメントは
同じセル内の他のセルへの接続関係の集合を持つ。樹状突起セグメントはある時点
でのネットワークの状態を理解する手段とするため、これらの接続を形成している。
周囲のアクティブなセルは数百から数千あるかも知れないが、
樹状突起セグメントが接続しなければならないのはこのうちの 15 から 20 程度に過ぎない。
ry 15 個のセルがアクティブと分かれば、その大きなパターンが発生 ry ほぼ確信できる。
このテクニックを「サブサンプリング」と呼び、HTM アルゴリズム全体を通じて利用している。

各セルは多くの異なる分散パターンに関与し、また多くの異なるシーケンスに関与 ry
。ある特定のセルは数十から数百の時間的遷移に関与しているかも ry
。従って各セルは一つではなく、いくつかの樹状突起セグメントを持つ。
ry セルが理解したいアクティビティの各パターンごとに一つの樹状突起セグメントを持つこ



Page 38

とが望ましい。しかし ry 樹状突起セグメントはいくつかの完全に異なるパターンに関して
接続を学習することができ、それでもうまく行く。
例えば、一つのセグメントが 4 つの異なるパターンのそれぞれについて 20 ry 都合 80 個
の接続を持つとする。そして、これらの接続のうち任意の 15 個がアクティブ
なときに樹状突起セグメントがアクティブとなるようにしきい値を設定する。
これにより誤りが発生する可能性が生じる。異なるパターンが混在することで、
樹状突起セグメントのアクティブな接続に関する 15 個のしきい値に到達する可能性がある。
しかしながら、表現の疎な性質により、このような誤りは非常に起こりにくい。

では、10 個から 20 個の樹状突起セグメントを持つセルと数千個のシナプスが
どのようにして数百種類のセルのアクティブ状態を理解するのかを見ていこう。

時間プーリングの詳細

ry 行われるステップを数え上げていく。
空間プーリングを終えてフィード・フォワード入力を表現するアクティブなカラムの集合
が得られたところから始める。

1) それぞれのアクティブなカラムについて、カラムの中のセルで予測状態のものを調べ、
アクティブにする。すべてのセルが予測状態でないなら、カラム中のすべての
セルをアクティブにする。結果として得られたアクティブなセルの集合は、
以前の入力の文脈の下での入力表現である。

2) リージョンのすべてのセルの各樹状突起セグメントについて、アクティブなセルに
接続されている接続状態のシナプスの数 ry 数がしきい値を超えていれば、
その樹状突起セグメントをアクティブとして印を付ける。
アクティブな樹状突起セグメントを持つセルを、
それがフィード・フォワード入力によって既にアクティブでない限り、予測状態にする。
アクティブな樹状突起を持たず、
フィード・フォワード入力によりアクティブになっていないセルは、非アクティブにする。
以上により、予測状態のセル全体がそのリージョンの予測となる。

3) 樹状突起セグメントがアクティブになったとき、そのセグメント上の
すべてのシナプスの永続値を更新する。そのアクティブな樹状突起セグメントの
すべてのシナプス候補について、アクティブなセルに接続しているシナプスの永続値を
増加させ、非アクティブなセルに接続しているシナプスの永続値を減少させる。
シナプスの永続値に対するこれらの変更に一時的と印を付ける。
これはセグメントをアクティブにし、
従ってまた予測をするほど既に十分に訓練されたシナプスを更新する。
しかしながら、可能であればさらに時間をさかのぼって



Page 39

予測ができるように常に拡張したい。
このため、同じセルの二番目の樹状突起セグメントを取り上げ、訓練する。
二番目のセグメントとして、
以前の時刻ステップのシナプスの状態に最もマッチするものを一つ選択する。
このセグメントに対して、
以前の時刻ステップのシステムの状態を用いて、アクティブなセルに接続している
シナプスの永続値を増加させ、非アクティブなセルに接続しているシナプスの永続値を
減少させる。シナプスの永続値に対するこれらの変更に一時的と印を付ける。

4) あるセルがフィード・フォワード入力によって予測状態からアクティブ状態41に変化
したときはいつも、そのセルに関連付けられているすべてのシナプス候補の「一時的」の印
を削除する。従ってフィード・フォワードによってセルがアクティブ化したことを
正しく予測したときだけ、シナプスの永続値を更新する。

5) セルがアクティブ状態から非アクティブ状態に変化したとき、このセルのすべての
シナプス候補について一時的な永続値の変更を元に戻す。
フィード・フォワードによってセルがアクティブ化したことを間違って予測したとき
はシナプスの永続値を強化したくないため。

フィード・フォワードによってアクティブになったセルだけを処理するのは
リージョンの内部だけであって、それ以外では予測はさらなる予測を引き起こす ry
。しかし（フィード・フォワードと予測の）すべてのアクティブなセルは
リージョンの出力となり、階層構造の次のリージョンへと引き継がれる。

一次と可変長42のシーケンスと予測

ry

一つのカラムに対するセルの数を増 ry 減 ry の効果 ry ？
特に、１カラムに１つのセルしかないとき ry ？

以前用いた例では、カラム当たり４セルのアクティブなカラムが 100 個の場合、
入力の表現は 4100 通りの異なるコード化が可能 ry
。従って、同じ入力が様々な文脈の中で出現しても混乱しない ry
。例えば、もし入力パターンが単語を表すなら、リージョンは
同じ単語が何度も使われる多くの文章を混乱することなく

41 原文は“inactive to active”となっているが、web 上の forum で
“predictive state to active state”の間違いだったとの訂正があった。
(2010/12/14 Sabutai: title “Cortical Algorithms document: praise and suggestions”)
42 “first order” と “variable order”。前者は一つだけの長さのシーケンスと予測、
後者は任意の長さのシーケンスと予測。



Page 40

記憶できる。「犬」のような単語が異なる文脈の中でユニークな表現を持つ ry
。この能力により HTM リージョンは可変長の予測 ry

可変長予測は現在起きていることだけではなく、可変の長さの過去の文脈に基づいて予測する。
HTM リージョンは可変長の記憶である。

カラム当たり 5 セルに増やすと、任意の特定の入力に対して可能なコード化の数は
5100 に増加し、4100 よりずっと大きくなる。しかしこれらの数は両方とも非常に大きく、
多くの現実的な問題においてこの容量の増加はあまり役に立たないだろう。

しかしながら、カラム当たりのセルの数をこれより少なくすると、大きな違いが生まれる。

もしカラム当たり１セルまでになると、文脈の中で表現する能力を失 ry
リージョンへの入力は、以前の活動に関係なく常に同じ予測を引き起 ry
。カラム当たり１セルの場合、HTM リージョンの記憶は一次記憶となり、
予測は現在の入力だけに基づ ry

一次予測は脳が解くことのできるある種の問題 ―静的空間推論― に理想的である。
ry 目が後を追うには短かすぎる時間であっても ry
聞く ry 理解するには常にパターンのシーケンスを聞く必要がある。
視覚も普通はそれに似ていて、視覚的イメージの流れを処理する必要がある。
しかしある条件下では、一瞬 ry

時間的理解と静的理解とでは、異なる推論メカニズム ry
一方は可変長の文脈に基づいてパターンのシーケンスを理解し、予測をする必要がある。
他方は時間的文脈を使わずに静的な空間的パターンを理解する必要がある。
カラム当たり複数のセルを持つ HTM リージョンは時間に基づくシーケンスを理解
するのに理想的であり、カラム当たり１セルの HTM リージョンは空間的パターンを理解
するのに理想的である。
Numenta では、カラム当たり１セルのリージョンを視覚問題に適用した実験を数多く実施 ry
重要な概念だけ述べる。

HTM リージョンにイメージを入力すると、リージョン内のカラムは画素の共通の空間的配列
の表現を学習する。学習するパターンの種類は新皮質の V1 野
（生物学で広く研究されている新皮質のリージョン）で観察されるものと似ていて、
概ね、異なる角度の線と角である。動画像 ry 、これらの基本的な形の遷移を学習する。
例えばある箇所に垂直な線があって、左又は右に移動した垂直な線がそれに続く ry
よく観察されるパターンの遷移は HTM リージョンで記憶される。 



Page 41

もしリージョンへの入力画像が、垂直な線が右に移動するものだったら ry ？
カラム当たり１セルしかなかったら、線が次に左又は右に現れること43を予測できる ry
。線が過去にどこにあったか知っているという文脈を使うことができないため、
それが左ないし右に移動していることを知ることはできない。
このようなカラム当たり１セルのものは、新皮質の「複雑型細胞」44のように振舞う ry
。そのようなセルの予測出力は、その線が左や右に動いていようがいまいが
異なる位置にある視覚的な線に対してアクティブになるだろう。
このようなリージョンは異なるイメージを区別する能力を保持する一方で、
平行移動や大きさの変化に対して安定性を示す ry 。このような振る舞いは、
空間的不変性（同じパターンの異なる見方を理解すること）のために必要である。

もし同じ実験をカラム当たり複数のセルを持つ HTM リージョンに対して行えば、
そのセルが新皮質の「方位選択性複雑型細胞」45のように振舞う ry
。セルの予測出力は左に移動する線や右 ry 線に対してアク ry になるが、両方に対しては ry

これらをまとめ ry 仮説 ry
新皮質は一次と可変長の両方の推論及び予測をしなければならない。
新皮質の各リージョンには 4 又は 5 層のセルがある。
層は様々な形に異なっているが、それらはすべてカラム単位で応答する性質を共有しており、
その層内で水平方向に大きな接続性を持っている。
新皮質のセルの層はそれぞれ、この章で述べたような HTM の推論と学習に似たことを
実行しているのではないか ry
。異なる層のセルは異なる役割 ry 。例えば解剖学によれば
第 6 層は階層構造のフィードバックを形成し、第 5 層は運動の動作に関わっている。
セルの 2 つの主要なフィード・フォワード層は第 4 層と第 3 層である。
第 4 層と第 3 層の一つの違いは、第 4 層のセルが独立に、即ちカラムの中で１セルだけが
動作するのに対して、第 3 層のセルはカラムの中で複数のセルが動作 ry
。よってセンサ入力に近い新皮質のリージョンは一次記憶と可変長記憶の両方を持つ。
一次シーケンス記憶（だいたい第４層のニューロンに対応する）は空間的に不変の表現を
形成するのに役立つ。可変長シーケンス記憶（だいだい第 3 層のニューロンに対応する）
は動画像の推論と予測に役立つ。

まとめ ry 章で述べたようなアルゴリズムは新皮質のニューロンのすべての層 ry 仮説 ry
。新皮質の層の詳細は大きく違っていて、
フィード・フォワードとフィードバック、注意46、運動動作47に関する異なる役割を演じ

43 「移動する」ことは予測できないが、隣の位置に「出現する」ことは予測できるということ。
44 complex cell
45 directionally-tuned complex cell
46 attention
47 motor behavior



Page 42

ている。センサ入力に近いリージョンでは、
一次記憶を実行するニューロンの層が空間的不変性に有利であるため役に立つ。

ry 一次（カラム当たり１セル）の HTM リージョンを画像認識問題に適用する実験 ry
。我々はまた、可変長（カラム当たり複数セル）の HTM リージョンに
可変長のシーケンスを理解・予測させる実験をした。
将来 ry 一つのリージョンに混在させ、他の目的にもアルゴリズムを拡張 ry
しかしながら、一つの層と等価なカラム当たり複数セルの構造が、
単体であれ複数階層であれ、多くの興味深い問題を取り扱いうる ry



Page 43

第３章： 空間プーリングの実装と疑似コード

本章では空間プーリング関数48の最初の実装の疑似コードを詳細に示す。
このコードの入力は、センサー・データ又は前のレベルからのバイナリ配列である。
このコードは activeColumns(t) を計算する。activeColumns(t) は時刻 t において、
フィード・フォワード入力に対して選択されたカラムのリスト ry
時間プーリング関数の入力 ry activeColumns(t) は空間プーリング関数の出力 ry

疑似コードは３つのフェーズ ry 順に実行 ry

ry 1: 各カラムについて、現在の入力のオーバラップを計算する。
ry 2: 抑制の後に勝者となったカラムを計算する。
ry 3: シナプスの永続値と内部変数を更新する。

空間プーリングの学習はオンライン49で行われるが、
フェーズ 3 を単にスキップすることで学習をしないようにすることもできる。

以下、３つのフェーズのそれぞれについて疑似コードを示す。
ry 様々なデータ構造や補助関数は本章の最後に示す。

初期化
最初の入力を受け取る前に、各カラムの最初のシナプス候補のリストを計算して
リージョンを初期化する。
これは入力配列の中からランダムに選択された入力位置のリストで構成される。
各入力はシナプスで表現され、ランダムな永続値が割り当てられる。
ランダムな永続値は二つの条件を満たす ry
。第一に、その値は connectedPerm
（シナプスが「接続している」と判定される最小の永続値）の前後の狭い範囲 ry 
。これにより、訓練を少ない回数繰り返しただけで、 ry 接続（ないし切断） ry
。第二に、各カラムは入力リージョン上で自然な中心位置があり、
永続値はこの中心に向かってバイアスを加えられる。（中心付近ではより高い値 ry ）

48 spatial pooler function
49 online。推論の計算と学習の計算を分離せずに、同時 ry



Page 44

フェーズ 1: オーバラップ
ry 与えられた入力ベクトルについて、そのベクトルと各カラムのオーバラップを計算する。
ry オーバラップは、アクティブな入力と接続されたシナプスの数 ry にブースト値を掛け ry
。もしこの値がminOverlap を下回 ry 0 ry

1. for c in columns
2. 
3. 	overlap(c) = 0
4. 	for s in connectedSynapses(c)
5.	 	overlap(c) = overlap(c) + input(t, s.sourceInput)
6. 
7. 	if overlap(c) < minOverlap then
8. 		overlap(c) = 0
9. 	else
10. 		overlap(c) = overlap(c) * boost(c)

フェーズ 2: 抑制
ry 抑制の後に勝者となったカラムを計算する。
desiredLocalActivity は勝者となるカラムの数を制御するパラメータである。
例えば、 desiredLocalActivity を 10 ry
抑制半径 ry においてカラムのオーバラップ値が高い順に 10 位以内のカラムが勝者 ry

11. for c in columns
12. 
13. 	minLocalActivity = kthScore(neighbors(c), desiredLocalActivity)
14. 
15. 	if overlap(c) > 0 and overlap(c) minLocalActivity then
16. 		activeColumns(t).append(c)
17. 

フェーズ 3: 学習
ry 学習を実行する。 ry すべてのシナプスの永続値は必要に応じて更新され、
ブースト値と抑制半径を更新する。 



Page 45

主要な学習規則は 20-26 行 ry 。勝者となったカラムのそれぞれについて、
もしあるシナプスがアクティブであればその永続値をインクリンメントし、
その他の場合はデクリメントする。永続値は 0 から 1 の範囲 ry

28-36 行目ではブーストを実装している。
カラムが接続を学習するための二つの独立したブースト機構がある。
あるカラムがあまり勝者となっていない（activeDutyCycleで観測される）とき、
そのブースト値をインクリメントする（30-32 行目）。
一方、あるカラムのシナプスがどの入力ともあまりオーバラップしない50
（overlapDutyCycle で観測される）とき、その永続値がブーストされる（34-36 行目）。
ノート：学習モードがオフになると、ブースト値は固定される。

フェーズ 3 の最後に、抑制半径を再計算する（38 行目）。

18. for c in activeColumns(t)
19. 
20. 	for s in potentialSynapses(c)
21. 		if active(s) then
22. 			s.permanence += permanenceInc
23. 			s.permanence = min(1.0, s.permanence)
24. 		else
25. 			s.permanence -= permanenceDec
26. 			s.permanence = max(0.0, s.permanence)
27. 
28. for c in columns:
29. 
30. 	minDutyCycle(c) = 0.01 * maxDutyCycle(neighbors(c))
31. 	activeDutyCycle(c) = updateActiveDutyCycle(c)
32. 	boost(c) = boostFunction(activeDutyCycle(c), minDutyCycle(c))
33. 
34. 	overlapDutyCycle(c) = updateOverlapDutyCycle(c)
35. 	if overlapDutyCycle(c) < minDutyCycle(c) then
36. 		increasePermanences(c, 0.1*connectedPerm)
37. 
38. inhibitionRadius = averageReceptiveFieldSize()
39. 

50 オーバラップ値が小さい



Page 46

データ構造と補助関数

以下の変数とデータ構造が疑似コードで ry

columns
すべてのカラムのリスト
input(t,j)
時刻t におけるこのレベルへの入力。j 番目の入力がオンのとき、input(t, j) は1である。
overlap(c)
ある入力パターンに対する、カラムc の空間プーリング・オーバラップ
activeColumns(t)
フィード・フォワード入力により勝者となったカラムの添え字のリスト
desiredLocalActivity 抑制ステップの後に勝者となるカラムの数を制御するパラメータ
inhibitionRadius
カラムに接続された受容野51のサイズの平均値
neighbors(c)
カラムc から inhibitionRadius の範囲内にあるすべてのカラムのリスト
minOverlap
抑制ステップで処理対象となるべきカラムのアクティブな入力の最小の数52
boost(c)
学習のときに計算される、カラムc のブースト値。
この値はアクティブでないカラムのオーバラップ値を増加させるために使われる。
synapse
シナプスを表すデータ構造。永続値と接続元の入力の添え字からなる。
connectedPerm
もしあるシナプスの永続値がこの値よりも大きければ、接続していると判定される
potentialSynapses(c)
シナプス候補とその永続値のリスト
connectedSynapses(c)
potentialSynapses(c) の部分集合で、永続値がconnectedPerm以上のものからなる。
これらは現在カラムc に接続されているフィード・フォワード入力である。
permanenceInc
学習時にシナプスの永続値を増加させる増分値
permanenceDec
減少値

51 巻末の用語の説明参照
52 あるカラムへのアクティブな入力がこの数以上であれば、抑制ステップで処理対象となる。



Page 47

activeDutyCycle(c)
抑制の後にカラムc がアクティブになった頻度を表す移動平均値
（例えば直近1000回繰り返した間にアクティブになった回数）
overlapDutyCycle(c) カラムc がその入力に対して有意なオーバラップ値（即ち、
minOverlapより大きな値）になった頻度を表す移動平均値
（例えば直近1000回繰り返した間に有意なオーバラップ値になった回数）
minDutyCycle(c)
最小限望まれるセルの発火頻度を表す変数。
あるセルの発火頻度がこの値を下回れば、それはブーストされる。
この値はその付近のカラムの最大の発火頻度の1%として計算する。
以下の補助関数が疑似コードで使用されている。
kthScore(cols, k)
与えられたカラムのリストに対して、k番目のオーバラップ値を返す
updateActiveDutyCycle(c)
抑制の後にカラムc がアクティブになった頻度の移動平均を計算する
updateOverlapDutyCycle(c)
カラムc のオーバラップ値がminOverlap より大きくなった頻度の移動平均を計算する
averageReceptiveFieldSize()
カラムに接続された受容野の半径についての、すべてのカラムの平均値。
カラムに接続された受容野は接続されたシナプス（永続値≧connectedPermのもの）
だけが含まれる。これはカラム間の横方向の抑制範囲を決めるために用いられる。
maxDutyCycle(cols) 与えられたカラムのリストのうち、activeDutyCycle が最大のものを返す
increasePermanences(c, s)
カラムc のすべてのシナプスの永続値をスケール因子s に従って増加させる
boostFunction(c)
カラムc のブースト値を返す。ブースト値は1以上のスカラ値である。
activeDutyCycle(c) が minDutyCycle(c) より大きければブースト値は1。
activeDutyCycle が minDutyCycle より下回り始めて以降は、ブースト値はリニアに増加する。



Page 48

第４章： 時間プーリングの実装と疑似コード

ry 時間プーリング関数53の最初の実装の疑似コード ry
ry 入力は空間プーリング関数で計算した activeColumns(t) である。
このコードは時刻 t における各セルのアクティブ状態及び予測状態を計算する。
アクティブ状態と予測状態の論理和が ry 出力 ry 、次のレベルの入力 ry

疑似コードは３つのフェーズ ry 順に実行 ry

ry 1: 各セルについてアクティブ状態 activeState(t) を計算する。
ry 2: 各セルについて予測状態 predictiveState(t) を計算する。
ry 3: シナプスを更新する。

ry 3 は学習するときにだけ必要 ry しかし ry 空間プーリングのときとは異なり、
学習が有効のときはフェーズ 1 と 2 も学習特有の操作をいくつか含んでいる。
時間プーリングは空間プーリングよりかなり複雑であるため、
先ずは時間プーリングの推論だけのバージョン ry 次に推論と学習を含む ry
。補助関数は疑似コードの後に、本章の最後に示す。

時間プーリング疑似コード： 推論だけのバージョン

フェーズ 1
ry 各セルのアクティブ状態を計算する。勝者となった各カラムについて、
どのセルをアクティブにするのかを決定する。
フィード・フォワード入力がいずれかのセルによって予測された
（即ち、前回の時刻ステップで順序セグメントによって predictiveState が 1 になった）
とき、それらのセルをアクティブにする（4-9 行目）。
フィード・フォワード入力が予測されなかった（どのセルも predictiveState がオンに
ならなかった）とき、そのカラムのすべてのセルをアクティブにする（11-13 行目）。

53 temporal pooler function



Page 49

1. for c in activeColumns(t)
2. 
3. 	buPredicted = false
4. 	for i = 0 to cellsPerColumn - 1
5. 		if predictiveState(c, i, t-1) == true then
6. 			s = getActiveSegment(c, i, t-1, activeState)
7. 			if s.sequenceSegment == true then
8. 				buPredicted = true
9. 				activeState(c, i, t) = 1
10. 
11. 		if buPredicted == false then
12. 			for i = 0 to cellsPerColumn - 1
13. 				activeState(c, i, t) = 1

フェーズ 2
ry 各セルの予測状態を計算する。
セルのどれかのセグメントがアクティブになると、そのセルの predictiveState がオンになる。
即ち、十分な数の横方向の接続先が、
フィード・フォワード入力によって現在発火していればオンになる。

14. for c, i in cells
15. 	for s in segments(c, i)
16. 		if segmentActive(c, i, s, t) then
17. 			predictiveState(c, i, t) = 1

時間プーリング疑似コード： 推論と学習を含むバージョン

フェーズ 1
ry 勝者となったカラム中の各セルの activeState を計算する。
それらのカラムについて、コードはさらにカラムごとに一つのセルを学習セル
(learnState)として選択する54。そのロジックは以下の通り。
フィード・フォワード入力がいずれかのセルによって予測された
（即ち、順序セグメントによって predictiveState

54 lcChosen は学習セルが選択されたこと（learn cell chosen）を表し、
(c, i) が選択された学習セル、そして learnState(c, i, t) =1 に設定されることで
このセルが学習セルとして選択されたことを記憶する。



Page 50

が 1 になった）とき55、それらのセルをアクティブにする（23-27 行目）。
そのセグメントが、learnState がオンのセルによってアクティブになった場合、
そのセルは学習セルとして選択される（28-30 行目）。フィード・フォワード入力が
予測されなかったとき、そのカラムのすべてのセルをアクティブにする（32-34 行目）。
さらに、ベストマッチセルが学習セルとして選択され（36-41 行目）、
新しいセグメントがそのセルに追加される。

18. for c in activeColumns(t)
19. 
20. 	buPredicted = false
21. 	lcChosen = false
22. 	for i = 0 to cellsPerColumn - 1
23. 		if predictiveState(c, i, t-1) == true then
24. 			s = getActiveSegment(c, i, t-1, activeState)
25. 			if s.sequenceSegment == true then
26. 				buPredicted = true
27. 				activeState(c, i, t) = 1
28. 				if segmentActive(s, t-1, learnState) then
29. 					lcChosen = true
30. 					learnState(c, i, t) = 1
31. 
32. 		if buPredicted == false then
33. 			for i = 0 to cellsPerColumn - 1
34. 				activeState(c, i, t) = 1
35. 
36. 		if lcChosen == false then
37. 			I,s = getBestMatchingCell(c, t-1)
38. 			learnState(c, i, t) = 1
39. 			sUpdate = getSegmentActiveSynapses (c, i, s, t-1, true)
40. 			sUpdate.sequenceSegment = true
41. 			segmentUpdateList.add(sUpdate)

55 buPredicted はフィード・フォワード入力が予測されたこと（ bottom-up predicted）を表す。
（bottom-up は feed-forward と同義で、「フィード・フォワード」と訳している）



Page 51

フェーズ 2
ry 各セルの予測状態を計算する。セルのどれかのセグメントがアクティブになると、
そのセルの predictiveState がオンになる。
即ち、十分な数の横方向の接続先が、フィード・フォワード入力によって現在アクティブ
であればオンになる。この場合、そのセルは以下の変更を待ち行列に加える：
a) 現在アクティブなセグメントを強化56する（47-48 行目）、
b) このアクティベーション57を予測し得たセグメント
（即ち、前回の時刻ステップでアクティビティに（弱いかもしれないが）マッチしたセグメント）
を強化する（50-53 行目）。

42. for c, i in cells
43. 	for s in segments(c, i)
44. 		if segmentActive(s, t, activeState) then
45. 			predictiveState(c, i, t) = 1
46. 
47. 			activeUpdate = getSegmentActiveSynapses(c, i, s, t, false)
48. 			segmentUpdateList.add(activeUpdate)
49. 
50. 			predSegment = getBestMatchingSegment(c, i, t-1)
51. 			predUpdate = getSegmentActiveSynapses(
52. 				c, i, predSegment, t-1, true)
53. 			segmentUpdateList.add(predUpdate)

フェーズ 3
第 3 の、そして最後のフェーズでは、実際に学習を実施する。
待ち行列に追加されたセグメントの更新は、フィード・フォワード入力を得てセルが
学習セルとして選択されたときに実施される（55-57 行目）。
そうではなく、もしセルが何らかの理由で予測を停止した場合、
そのセグメントをネガティブ58に強化する（58-60 行目）。

56 reinforcement。フェーズ 3 で強化学習をする処理対象を segmentUpdateList に記憶する。
57 44 行目の if 文の条件が成立したこと
58 逆に弱める方向に学習する



Page 52

54. for c, i in cells
55. 	if learnState(s, i, t) == 1 then
56. 		adaptSegments (segmentUpdateList(c, i), true)
57. 		segmentUpdateList(c, i).delete()
58. 	else if predictiveState(c, i, t) == 0 and predictiveState(c, i, t-1)==1 then
59. 		adaptSegments (segmentUpdateList(c, i), false)
60. 		segmentUpdateList(c, i).delete()
61. 

実装の詳細と用語説明

ry 。各セルは二つの数値でアクセスする。カラムの添字 c と、セルの添字 i である。
セルは樹状突起セグメントのリストを保持する。各セグメントはシナプスのリストと、
各シナプスごとに永続値を保持する。セルのシナプスに対する変更は、
セルがフィード・フォワード入力によってアクティブになるまでは一時的とマークされる。
これらの一時的な変更はsegmentUpdateList によって保持される。
各セグメントはまた、論理値のフラグsequenceSegment を保持する。
これはそのセグメントが次の時刻ステップにおけるフィード・フォワード入力
を予測するかどうかを示している。

シナプス候補の実装は空間プーリングの実装とは異なっている。
空間プーリングでは、シナプス候補の完全なリストが明示的に示される。
時間プーリングでは各セグメントが固有のシナプス候補の（ときには大きな）リスト
を保持することができる。実際には各セグメントごとに大きなリストを管理することは、
計算量が大きくメモリ消費が集中する。そこで我々は、時間プーリングでは学習の際に
各セグメントごとにアクティブなシナプスをランダムに追加する
（newSynapseCount パラメータで制御する）。
この最適化はすべてのシナプス候補のリストを維持管理するのと同様の効果があり、
しかも新たな時間的パターンを学習でき ry セグメント毎のリストはずっと小さくなる。

疑似コードはまた、異なる時刻ステップのセル状態の推移 ry 小さな状態遷移マシンを使用 ry
。各セルごとに三つの異なる状態を維持管理する。
配列activeState と predictiveState は各セルの各時刻ステップごとの
アクティブ状態及び予測状態の推移を追う。
配列 learnState はどのセルの出力が学習のときに使用されるかを決定する。
入力が予測されなかったときは、
その特定のカラムのすべてのセルが同じ時刻ステップ内に同時にアクティブになる。
これらのセルのうちの一つだけ（入力に最もマッチするセル）で learnState がオンになる。
learnState がオンのセルだけにつ



Page 53

いて、シナプスを追加する（これは
樹状突起セグメントの中で完全にアクティブになったカラムを強調しすぎないように ry ）。



Page 54

以下のデータ構造が時間プーリング疑似コードで ry

cell(c,i)
すべてのセルのリスト。iとcで指し示される。
cellsPerColumn
各カラムのセルの数
activeColumns(t)
フィード・フォワード入力によって勝者となったカラム（空間プーリング関数の出力）
のインデックスのリスト
activeState(c, i, t)
各セルが一つずつ持つ論理値のベクトル。カラムc セルi時刻tにおけるアクティブ状態を表す。
これは現在のフィード・フォワード入力と過去の時間的文脈から与えられる。
activeState(c, i, t) が1なら、そのセルは現在フィード・フォワード入力を持ち、
適切な時間的文脈を持つ。
predictiveState(c, i, t)
各セルが一つずつ持つ論理値のベクトル。カラムc セルi時刻tにおける予測状態を表す。
これは他のカラムのフィード・フォワード状態と過去の時間的文脈から与えられる。
predictiveState(c, i, t) が1なら、そのセルは現在の時間的文脈から
フィード・フォワード入力を予測している。
learnState(c, i, t)
カラムc セルi が学習対象のセルとして選択されたことを表す論理値
activationThreshold
あるセグメントをアクティブにするしきい値。 ry 接続されたアクティブなシナプスの数が
activationThreshold より大きければ、セグメントはアクティブになる。
learningRadius
横方向の接続を持つ、時間プーリングセルの周囲の領域の範囲
initialPerm
シナプスの永続値の初期値
connectedPerm
あるシナプスの永続値がこの値より大 ry 接続している ry
minThreshold
学習の際の、アクティブなセグメントの最小数
newSynapseCount 学習のときにセグメントに追加されるシナプスの最大数
permanenceInc
アクティビティによる学習が発生したとき、シナプスの永続値を増加させる量
permanenceDec
ry 減少させる量



Page 55

segmentUpdate
与えられたセグメントを更新するときに必要な情報を保持するデータ構造で、
以下の3項目からなる。a) セグメントの添字（新しいセグメントのときは-1）、
b) 既存のアクティブなシナプスのリスト、c) このセグメントが順序セグメントと
マークされべきかどうかを表すフラグ（デフォルト値はfalse）。
segmentUpdateList
segmentUpdate 構造体のリスト。segmentUpdateList(c,i) はカラムc セルi の
変更内容のリストである。
以下の補助関数が上記のコードで使用されている。
segmentActive(s, t, state)
セグメントs 時刻t において、state で与えられた状態によってアクティブになった接続状態
のシナプスの数がactivationThreshold より大きい時、真を返す。
state パラメータは activeState 又は learnState である。
getActiveSegment(c, i, t, state)
与えられたカラムc セルi について、segmentActive(s,t, state)が真になるセグメントの添字を返す。
もし複数のセグメントがアクティブであれば、順序セグメントがあればそれが優先する。
そうでなければもっともアクティビティが高いものが優先する。
getBestMatchingSegment(c, i, t)
与えられたカラムc セルi について、アクティブなシナプスが最も多いセグメントを探す。
この関数は積極的にベストマッチを見つける。
シナプスの永続値は connectedPerm より小さくても許される。
アクティブなシナプスの数はactivationThreshold より小さくても許される。
ただし、minThreshold より大きくなければならない。
この関数はセグメントの添字を返す。もしセグメントが見つからなかったら-1 を返す。
getBestMatchingCell(c)
与えられたカラムについて、ベストマッチセグメント（上記参照）を持つセルを返す。
ry マッチ ry なければ、最もセグメントの数が少ないセルを返す。



Page 56

getSegmentActiveSynapses(c, i, t, s,newSynapses=false)
セグメントs に対して提示された変更のリストを保持するsegmentUpdate 構造体を返す。
activeSynapses を、接続元のセルの時刻t における activeState 出力=1 であるような
アクティブなシナプスのリストとする（s=-1のときはセグメントが存在しないため ry 空 ry
）。 ry 。 newSynapses が真の時は、 newSynapseCount -count(activeSynapses) 個
のシナプスが activeSynapses に追加 ry
シナプスは、時刻t において learnState出力=1 であるセルの中からランダム ry
adaptSegments(segmentList,positiveReinforcement)
この関数は segmentUpdate のリスト要素について繰り返し、各セグメントを強化 ry
。 ry e の各要素について、以下 ry
positiveReinforcement が真のとき、アクティブなリスト上のシナプスの永続値は
permanenceInc だけ増加させる。他 ry permanenceDec だけ減 ry
。 positiveReinforcement が偽のとき、アクティブなリスト上のシナプスは
永続値を permanenceDec だけ減 ry
。これらの処理の後、segmentUpdate の中にまだ存在しないシナプスについて、
永続値 initialPerm にて追加する。



Page 57

付録 A: 生体ニューロンと HTM セルの比較

上の画像は左側が生体ニューロンの写真、中央が単純な人工ニューロン、
右側がHTM のニューロンあるいはセル ry 。この付録の目的 ry 比 ry

実物のニューロンは途方もなく複雑で変化に富んでいる。
ここではその最も普遍的な原理に注目し、また我々のモデルに関わる部分に限定する。
実物のニューロンの多くの詳細は無視するものの、HTM 大脳皮質性学習アルゴリズムで
用いられているセルは多くのニューラルネットワークで用いられている人工ニューロンよりも
はるかに現実に即している。 ry

生体ニューロン

ニューロンは脳内で情報を伝えるセル59である。上記左の画像は標準的な興奮性の
ニューロンである。 ry 外見の大部分は枝分かれした樹状突起で占められている。
ry すべての興奮性の入力は、樹状突起に沿って配置されたシナプスを経由 ry
。近年、ニューロンに関する知識 ry 最大の発見は、
ニューロンの樹状突起が入力を細胞体60に繋ぐ単なる導線ではないと分かったことで

59 cell を本書では一貫して「セル」と訳したが、生物の細胞を意味する。
60 cell body。ニューロン中央の膨らんだ部分。



Page 58

ある。今では樹状突起はそれ自体、複雑な非線形処理部品であることが分かっている。
HTM 大脳皮質性学習アルゴリズムはこの非線形の特性を利用している。

ニューロンはいくつかの部分 ry

細胞体
細胞体はニューロンの中央にある小さな体積を持つ部分である。
セルの出力、軸索61は、細胞体から出ている。
セルへの入力は樹状突起に沿って配置されたシナプスであり、それは細胞体に入力される。

主要樹状突起62
細胞体に最も近い ry 枝は主要樹状突起 ry 図 ry いくつかを緑色の線 ry

ry 突起上の複数のアクティブなシナプスは、細胞体に対して概ね線形の加算 ry
。5 つのアクティブなシナプスは 1 つのアク ry 概ね 5 倍の脱分極63を細胞体に ry 引き起 ry
。対照的に、ある一つのシナプスが後続の素早い活動電位64によって繰り返しアクティブ
になったとしても、2 番目、3 番目と続く活動電位による細胞体への影響は、
最初のものよりずっと小 ry

このため、主要樹状突起への入力は細胞体に対して線形に加わること、単一のシナプスに
届いた複数の素早いパルスの影響は一つのパルスより少し大きいだけであることが言える。

新皮質のリージョンへのフィード・フォワード接続は主要樹状突起に優先的に接続 ry
。これは少なくとも、各リージョンの ry 主要な入力層である第 4 層のニューロンについて ry

末梢樹状突起65
細胞体から遠く ry 枝は末梢樹状突起 ry 図 ry いくつかを青色の線 ry

61 axon
62 proximal dendrite
63 depolarization。神経細胞内の電位は通常は-70~-60mV 程度。
ニューロンが刺激を受けたためにこの電位が上がることを脱分極という。
脱分極により電位があるしきい値を超えるとニューロンが発火する。
64 action potential。なんらかの刺激に応じて細胞膜に生じる一過性の膜電位の変化。
65 distal dendrite



Page 59

末梢樹状突起は主要樹状突起よりも細い。これらは樹状突起の木の中の他の樹状突起の枝に
接続されていて、細胞体に直接接続されていない。これらの違いにより末梢樹状突起は
ユニークな電気・化学特性を持っている。
末梢樹状突起で一つのシナプスがアクティブになっても、細胞体に対して最小限の影響しか ry
。シナプスで局所的に発生した脱分極は、それが細胞体に届くときには弱くなっている。
このことは長年のなぞであった。
ニューロンのシナプスの大多数を占める末梢のシナプスはあまり多くのことを ry

今では末梢樹状突起の各断片が半独立の処理領域として働くことが分かっている。
もしその樹状突起の短い区間内で十分な数のシナプスが同時にアクティブになると、
樹状突起のパルスを生成することができ、
それは細胞体にまで届いて大きな影響を及ぼすことができる。
例えば 40μm 間隔の 20 個のアクティブなシナプスは樹状突起のパルスを生成 ry

従って、末梢樹状突起は域内同時発生事象の検出器 ry

末梢樹状突起上のシナプスは、圧倒的にそのリージョン内の付近の他のセルから形成 ry

画像では上方向に伸びる大きな樹状突起の枝 ry 先端樹状突起66 ry
。ある理論 ry 、この構造によりニューロンは付近にいくつかの末梢樹状突起を形成し、
この付近を通る軸索により容易に接続 ry 。
この解釈によれば、先端樹状突起はセルの延長として働く ry

シナプス
標準的なニューロンには数千個のシナプスがある。これらの大多数(多分 90%)は
末梢樹状突起にあり、残りは主要樹状突起にあると思われる。

長年に渡って、学習はシナプスの影響度ないし「重み」を強くしたり弱くしたり
することを含むものと仮定されてきた。
このような影響は観測されるものの、各シナプスはどこか確率的な様子である。
アクティブになったとき、それが神経伝達物質67を放出することに信頼性がない。
よって脳が ry 各シナプスの重みの精度や忠実度に依存しているはずがない。

さらに今では、シナプス全体が素早く形成されたり切断 ry
。この柔軟性は学習の強力な表現形式であり、素早く知識を獲得 ry
。シナプスは軸索と樹状突起がある距離の範囲内にあるときにだけ形成されうることから、
シナプス「候補」の概念 ry 、学習は主にシナプス候補から有効なシナプスが形成 ry

66 apical dendrite。apical は「頂点の，頂上の」という意味。
67 neurotransmitter



Page 60

ニューロンの出力
ニューロンの出力は軸索に沿って伝搬するパルス68あるいは「活動電位」である。
軸索はセルから伸びて、ほとんど常に二つに分かれる。
枝の一つは水平に伸びて近くの他のセルと数多く接続する。
他の枝は他の層のセルや脳内の他の場所へと遠く伸びる。
上記のニューロンの画像では軸索は見えない。２本の矢印を追加して軸索を示した。

ニューロンの実際の出力は常にパルスであるが、この解釈には異なる見方ができる。
有力 ry （特に新皮質に関しては）、パルスの発生頻度が重要というもの ry
。よってセルの出力はスカラー値と見なすことができる。

いくつかのニューロンは数個の連続したパルスを短時間に素早く出力する「バースト」反応
を示すことも ry

ry 上記 ry HTM セルの特徴に関連する属性に注目して述べ ry 詳細は無視 ry
ここで述べたすべての特徴が幅広く受け入れられているとは必ずしも言えない。
ry 、我々のモデルにとって必要 ry

単純な人工ニューロン

この付録の最初に示した中央の図は、多くの古典的な ry に模した構成要素 ry
これらの人工ニューロンはシナプスの集合を持ち、各シナプスはウェイトを持っている。
各シナプスはスカラー値のアクティブ化を受け取り、それにシナプスのウェイトが掛け合 ry
。すべてのシナプスの出力は非線形の方法で足し合わされ、人工ニューロンの出力となる。
学習はシナプスのウェイトを調整 ry 恐らく非線形の関数 ry
。このタイプの人工ニューロン、そしてそのバリエーションは、
コンピュータを利用した価値ある道具として多くのアプリケーションにおいて有益 ry
しかし ry 生体ニューロンの多くの複雑さを捉えておらず ry 能力を活用していない。
ry もっと精巧なニューロンモデルが必要 ry

68 spike。短時間の尖った波形のこと。pulse とは少し違うが、パルスと訳した。



Page 61

HTM セル

図の右側は、HTM 大脳皮質性学習アルゴリズムで用いられているセル ry
HTM セルは実物のニューロンの多くの重要な能力を捉えているが ry 単純化も ry

主要樹状突起
各 HTM セルは単一の主要樹状突起を持つ。セルへのすべてのフィード・フォワード入力
はシナプス（緑色の点で示した）経由で入力される。アクティブなシナプスの効果は
線形に加算され、フィード・フォワード入力によるセルのアクティブ化を生み出す。

我々は、カラム内のすべてのセルが同じフィード・フォワード応答を持つよう求めている。
実物のニューロンではこれは恐らく抑制タイプのセルによって行われる。
HTMでは我々は単にカラム中のすべてのセルが単一の主要樹状突起を共有するように強制 ry

隣のセルとの競合に決して勝つことができないようなセルができることを避けるため、
HTM セルはその隣と比較して十分に勝利していないときには、そのフィード・フォワード入力
によるアクティブ化をブーストする。よってセル間には常に競合 ry
我々はこれをセル間ではなくカラム間の競合として HTM でモデル化 ry 図では示されていない。

最後に、主要樹状突起はリージョンへのすべての入力の部分集合となるシナプス候補の集合を
一つ持っている。セルが学習すると、その主要樹状突起上のすべてのシナプス候補の
「永続」値を増加ないし減少させる。しきい値を超えたシナプス候補だけが有効となる。

ry 生物学では、シナプスを形成するのに十分に近くにある軸索と樹状突起とを意味する。
ry 拡張して、HTM セルで接続する可能性のある、より大きな集合を意味すること ry
。生体ニューロンの樹状突起と軸索は学習によって成長したり縮退 ry
よってシナプス候補の集合は成長に伴って変化する。
HTM セルのシナプス候補の集合を大きめにすることで、我々は概ね、
軸索や樹状突起の成長と同じ結果を得た。
シナプス候補の集合は図示されていない。

カラム間の競合、シナプス候補の集合による学習、あまり活用されていないカラムの
ブーストの組み合わせにより、HTM ニューロンのリージョンは脳に見られるのと同様の
強力な可塑性を得た。HTM リージョンは入力の変化に応じて各カラムが何を表す



Page 62

のかを（主要樹状突起上のシナプスの変更によって）自動的に調整し、
カラム数の増加ないし減少を自動的に調整 ry

末梢樹状突起
各 HTM セルは末梢樹状突起セグメントのリストを管理している。
各セグメントはしきい値検出器として働く。
ry アクティブなシナプス（冒頭の図で青い点で示した）の数がしきい値を超えると、
そのセグメントはアクティブになり、それに接続されたセルが予測状態になる。
セルの予測状態はアクティブなセグメントの OR ry

樹状突起セグメントは、以前のある時点で互いに同時にアクティブになった複数のセル
への接続を形成することでリージョンの状態を記憶する。
そのセグメントは、以前にフィード・フォワード入力によってアクティブになったセルの状態
を記憶する。よってそのセグメントはセルがアクティブになることを予測する状態を探す。
樹状突起セグメントの標準的なしきい値は 15 である。
ry 有効なシナプスが 15 個同時にアクティブになると、その樹状突起はアクティブになる。
数百から数千個の周囲のセルがアクティブになるかも知れないが、
15 個の接続だけで十分により大きなパターンを理解 ry

各末梢樹状突起セグメントはそれに関連付けられたシナプス候補の集合を持つ。
そのシナプス候補の集合はリージョン内のすべてのセルの部分集合である。
そのセグメントが学習 ry 、それらすべてのシナプス候補の永続値を増加ないし減少 ry
。しきい値を超えたシナプス候補だけが有効 ry

ある実装では、我々はセルあたり固定の数の樹状突起セグメントを用いた。
他のある実装では、訓練を通じてセグメントを追加ないし削除するようにした。
ry 。セルあたりの樹状突起セグメントの数を固定にすると、
同じセグメントに対していくつかの異なるシナプスの集合を保存することができる。
例えば、セグメント上に 20 個の有効なシナプスがあり、しきい値が 15 とする。
（一般に ry ノイズへの耐性 ry しきい値をシナプスの数よりも少 ry ）
これでそのセグメントは周囲のセルの特定の一つの状態を理解できるようになる。
もし周囲のセルのまったく異なる状態を表現する、20 個の他のシナプスを
その同じセグメントに追加 ry するとそのセグメントは、
あるパターンの 8 個のアクティブなシナプスと他のパターンの 7 個のアクティブなシナプスを
持つことで間違ってアクティブになるかも知れないので、エラーが起こる可能性 ry
。我々は実験的に、20 個の異なるパターンまでならエラーなしで一つのセグメントに保存 ry
従って十数個の樹状突起セグメントを持つ HTM セルは多くの異なる予測に関与 ry 



Page 63

シナプス
HTM セルのシナプスは二値の重みを持つ。ry
重みをスカラー値にすることを妨げるものは何もないが、
疎分散パターンを用い ry 今のところスカラー値の重みを使う必要性がない。

しかし ry セルは「永続値」というスカラー値を ry 学習を通じて調整する。
永続値 0.0 は有効でないシナプス候補を表し、 ry しきい値（標準的には 0.2）
を超える永続値は接続したばかりで容易に切断するシナプスを表す。
高い永続値、例えば 0.9 は、接続状態でしかも容易には切断しない ry

HTM セルの主要樹状突起セグメントや末梢樹状 ry にある有効なシナプス
の数は固定ではない。それはセルがパターンに触れるに従って変化する。
例えば、末梢樹状突起の有効なシナプスの数はデータの時間的構造に依存する。
リージョンへの入力に時間的に永続的なパターンが何もないときは、
末梢セグメントのすべてのシナプスは低い永続値を持ち、
わずかな数のシナプスだけが有効になるだろう。
入力列にたくさんの時間的構造があるときは、高い永続値を持つ有効なシナプスが多数 ry

セル出力
HTM セルは 2 つの異なる二値出力を持つ： 1) セルがフィード・フォワード入力によって
（主要樹状突起経由で）アクティブである、2) セルが横方向の接続により
（末梢樹状突起経由で）アク ry 。前者を「アクティブ状態」と呼び、後者を「予測状態」 ry

冒頭の図では、この 2 つの出力は正方形の細胞体から出ている 2 つの線で表されている。
左側の線はフィード・フォワードによるアクティブ状態、右側の線は予測状態である。

フィード・フォワードによるアクティブ状態だけがリージョン内の他のセルに接続され、
これにより予測は常に現在の入力（及び文脈）に基づいて行われる。
予測に基づいて予測が行われることは望ましくない。 ry
そうなると、数回処理を繰り返しただけでリージョン内のほとんどすべてのセルが予測状態 ry

リージョンの出力はすべてのセルの状態を表すベクトルである。
もし階層構造の次のリージョンがあるなら、このベクトルがその入力となる。
この出力はアクティブ状態と予測状態の OR である。
アクティブ状態と予測状態を結合する ry 出力は入力よりも安定する（ゆっくりと変化する）。
このような安定性はリージョンの推論における重要な特性である。 



Page 64

参考文献

ry 。ニューロサイエンス ry 膨大で、全般的知識を得るには多くの異なる文献 ry
。新しい発見は学術ジャーナル ry 、読み解くのが難しく大学関係者でない限り ry

この付録で述べた話題に関してより専門的 ry 2 冊 ry

Stuart, Greg, Spruston, Nelson, Hausser, Michael, Dendrites, second edition
(New York: Oxford University Press, 2008)

この本は樹状突起に関するあらゆる ry
16 章では HTM 大脳皮質性学習アルゴリズムが用いている樹状突起セグメントの
非線形な性質 ry この章は、この分野で数多くの考察をした Bartlett Mel によって書 ry

Mountcastle, Vernon B. Perceptual Neuroscience: The Cerebral Cortex
(Cambridge, Mass.: Harvard University Press, 1998)

この本は新皮質に関するあらゆることに関する良い入門 ry 細胞の種類とその接続関係 ry
樹状突起の性質 ry 得るには古 ry 、読者は皮質性ニューロンに関するすぐれた見識 ry 



Page 65

付録 B: 新皮質の層と HTM リージョンの比較

ここでは HTM リージョンと生体新皮質のリージョンの間の関係 ry

特に、HTM 大脳皮質性学習アルゴリズム、
及びそのカラムとセルが、新皮質の層やカラム構造とどのような関係 ry
。新皮質の「層」の概念やそれが HTM の層とどう関係 ry 、多くの人が困惑 ry
。本稿がこの混乱を解決し、また HTM 大脳皮質性学習アルゴリズムの生物学的基礎 ry

新皮質の神経回路網

人の新皮質は面積約 1,000cm2、厚さ 2mm のニューロンの皮である。
ry 食事に使うナプキンの布 ry 、新皮質の面積と厚さのちょうど良い近似 ry
。新皮質は十数種類の機能的なリージョン ry
、そのいくつかは視覚に関係し、あるいは聴覚、言語などに ry
。顕微鏡で見ると、異なるリージョンの物理的な特徴は驚くほど良く似ている。

新皮質全体を通じて各リージョンには器官原理69がいくつか見られる。

69 organizing principles。生体器官の働きの原理的しくみ。



Page 66

層
新皮質は一般に 6 つの層を持つと言われている。
それらの層のうち 5 つはセルを持ち、1 つの層はほとんどが接続線である。
層は染色技術の出現と共に 100 年以上前に発見された。
上記の画像（Cajal による）は 3 種類の異なる染色法を用いて新皮質の小さな断片 ry
垂直方向の軸索は約 2mm の新皮質の厚さ全体に及んでいる。
画像の左側は 6 つの層 ry 最上部の第 1 層はセルがない層である。
最下部の「WM」は白質が始まるところを示しており、
セルからの軸索はそこから新皮質の他の部分や脳の他の部分へと伸びている。
画像の右側は髄鞘を持つ軸索だけを示す染色法である。（髄鞘形成70とは
一部の軸索を覆っている脂肪質の鞘71である。ただしすべての軸索を覆っているのではない）
この部分の画像から新皮質の 2 つの主要な器官原理である、層とカラムを見 ry
。多くの軸索はニューロンの本体から出た直後

70 myelination。ずいしょうけいせい。ミエリン化。
71 sheath。さや。



Page 67

に 2 つに枝分かれする。枝の一つは主に水平に伸び、他の枝は主に垂直 ry
。水平の方の枝は同じ層や近くの層の他のセルと多数の接続を成し、
そのために染色でこのように層が見られる。これは新皮質の断片を表している ry
。多くの軸索がこの画像で示された部分から出たり入ったりしているので、
軸索は画像に見られるものよりも長い。
新皮質の 1mm の立方体に含まれる軸索や樹状突起の総延長は 2km から 4km ry

画像の中央部はニューロンの本体だけを示す染色法で、樹状突起や軸索は見えない。
ニューロンの大きさや密度が層によって変化する様子 ry
。この画像ではカラムは少ししか分からない。第 1 層にいくつかのニューロン ry
の数はあまりに少ないので、この層はやはりセルのない層 ry
。ニューロ科学者は新皮質の 1mm の立方体ごとに約100,000 個程度のニューロン ry

画像の左側はほんのわずかな数のニューロンの本体、軸索、樹状突起だけを示す染色法 ry
。異なる層や異なるセルごとに、樹状突起の「主軸」の大きさは異なっている様子 ry
。いくつかの「先端樹状突起」72 ry は細胞体からそびえ立ち、他の層と接続 ry
。先端樹状突起が存在するか否か、及びその接続先は各層ごとに特徴 ry

まとめると、新皮質の層とカラム構造73は ry 染色され顕微鏡で観察 ry 明らか ry

リージョンの違いによる層のバリエーション
新皮質のリージョンの違いによって層の厚さにバリエ ry 、層の数についても多少違う。
このバリエーションはどの動物を研究するかに依存し、
どのリージョンを観察するかにも依存し、また観察した人によっても違う。
例えば上記の画像では、第 2 層と第 3 層は容易に識別できるが一般的にはそうではない。
いくつかの科学者 ry この 2 つの層を識別できないと報告 ry まとめて「第 2/3 層」 ry
。他の科学者は逆の方向に向かい、例えば 3A と 3B のようなサブレイヤを定義 ry

第 4 層は、新皮質のリージョンの中で感覚器官にもっとも近い部分で最もよく定義されている。
いくつかの動物（例えばヒトやサル）では、第 1 視覚野の第 4 層は明確に細分化 ry
。他の動物ではそれは細分化されていない。
第 4 層は感覚器官から遠いリージョンでは階層構造から消えて無くなっている。

72 apical dendrite
73 columnar organization。一般には柱状構造。他と統一するためカラム構造とした。



Page 68

カラム
新皮質の 2 つ目の主要な器官原理はカラムである。いくつかのカラム構造は
染色された画像にも見られるが、カラムに関する多くの証拠は
異なる入力に対してセルがどのように反応するかに基づいている。

科学者が針を使って、何がニューロンをアクティブにするのかを見てみると、
異なる層を横断する垂直方向に揃った複数のニューロンがだいたい同じ入力に反応 ry


この図は、網膜からの情報を処理する最初の皮質性リージョンである V1 の、
セルのいくつかの応答特性 ry

最初の発見の一つは、V1 のほとんどのセルは網膜の特定の領域で、異なる角度の線や縁に反応 ry
。カラム状に垂直に配列された複数のセルすべてが、同じ角度の縁に反応する。
図を注意深く見れば、各区画の最上部に異なる角度の小さな線が描かれている ry
。これらの線はその場所のセルがどの角度の線に反応するかを示している。
垂直に配列された複数のセル（うすい垂直の縞模様の一つに含まれる）は同じ角度の線に反応する。

V1 にはいくつかの他のカラム型の特徴があり、そのうちの 2 つが図示されている。
左目と右目の情報の似た組み合わせにセルが反応する「眼球優位性カラム」74がある。

74 ocular dominance column。片方の眼からの入力に強く反応するセルの集まり。



Page 69

そしてセルが主に色を感知する「ブロブ」75がある。
眼球優位性カラムは図の大きなブロックである。各眼球優位性カラムは角度のカラムを含む。
「ブロブ」は濃い色の楕円である。

新皮質の一般的な規則は、角度と眼球優位性のようにいくつかの異なる応答特性が
互いに重ね合わさっているということである。皮質の表面を水平に移動してゆくに従って、
セルから出力される応答特性の組み合わせは変化する。
しかしながら、垂直に配列されたニューロンは同じ応答特性の組み合わせを共有している。
聴覚・視覚・体性感覚野についてはこのような垂直の配列になっている。
新皮質のあらゆる場所でそう ry 、全部ではなく多くの部分について言うならそれは正しい ry

ミニカラム
新皮質の最小のカラム構造はミニカラムである。ミニカラムは直径約 30μm で、
セルを持つ 5 つの層全体に及ぶ 80-100 個のニューロンが含まれている。
新皮質全体はミニカラムから構成されている。小さなスパゲッティのかけらを端同士を
積み重ねたものを思い浮かべるとよい。ミニカラムの間には
セルが少ししかないわずかなすきまがあり、染色された画像でそれを見ることができる。


左側は新皮質の一部の断片に見られるニューロンの細胞体を示す染色画像である。
ミニカラムの垂直の構造がこの画像から明白に分かる。
右側はミニカラムの概念図である（Peters と Yilmez による）。
実際にはこれよりずっと細い。カラムの中の各層に複

75 blob。小塊、小球体。



Page 70

数のニューロンがあることに注意してほしい。
ミニカラムのすべてのニューロンが類似の入力に反応する。
例えば、先ほど示した V1 の図では、ミニカラムは特定の眼球優位性を伴い、
特定の角度の線に反応するセルを含んでいる。隣にあるミニカラムのセルは
少し違う角度の線に反応し、違う眼球優位性を示すのかも知れない。

抑制ニューロンがミニカラムを定義する本質的な役割を果たしている。
ry 画像や図に示されていないが、抑制ニューロンはミニカラムの間のまっすぐな線に沿って
軸索を送っており、ミニカラムの一部を物理的に分離している。抑制ニューロンはまた、
ミニカラム中のニューロンが同じ入力に反応するよう強制することに役立っていると ry

ミニカラムは HTM 大脳皮質性学習アルゴリズムで用いられているカラムの原型である。

カラム反応の例外
カラム反応の例外が一つあって、それは HTM 大脳皮質性学習アルゴリズムにも関係する。
科学者は通常、実験動物に単純な刺激を与えることでセルが何に反応するのかを発見する。
例えば、動物の視覚空間の小さな部分に 1 つの線を見せて、V1 のセルの応答特性を調べ ry
。単純な入力を用いると、科学者はセルが常に同じ入力に反応することを発見するかも ry
。しかしながら、もしその単純な入力が自然な場面の動画像に組み込まれたなら、
セルはもっと選択的になる。あるセルが高い信頼性で独立した垂線に反応するとしても、
その垂線が自然な場面の複雑な動画像に組み込まれた場合は必ずしも反応するとは ry

HTM 大脳皮質性学習アルゴリズムでは
カラム中のすべての HTM セルが同じフィード・フォワード応答特性を共有しているが、
時間的なシーケンスを学習すると HTM カラムの 1 つのセルだけがアクティブになる。
このメカニズムは可変長シーケンスを表現する手段であり、
ニューロンについて先ほど説明した特徴と似ている。
文脈を伴わない単純な入力はカラム中のすべてのセルをアクティブにする。
同じ入力でも、学習したシーケンスに含まれるときは 1 つのセルだけがアクティブになる。
ミニカラム中で一度に 1 つのニューロンだけがアクティブになると提唱しているわけではない。
HTM 大脳皮質性学習アルゴリズムが提唱しているのは、
予期しない入力に対してはカラム中のある層のすべてのニューロンがアクティブになり、
予期した入力に対してはその一部のニューロンがアクティブ ry



Page 71

なぜ層とカラムがあるのか？

新皮質になぜ層があり、なぜカラムがあるのか、はっきりしたことは誰も知らない。
HTM 大脳皮質性学習アルゴリズムは、カラム状に構成したセルの層が
可変長の状態遷移を記憶する大容量メモリとなりうることを示した。
もっと単純に言えば、セルの層はたくさんのシーケンスを学習できる ry
。同じフィード・フォワード反応を共有するセルのカラムは可変長の遷移を学習 ry の鍵 ry

この仮説はなぜカラムが必要なのかを説明しているが、しかし 5 つの層については ry ？
もし 1 層の皮質でシーケンスを学習して予測できるのであれば、なぜ ry  5 つ ry ？

我々が提唱するのは、新皮質に観察される異なる層はすべて同じ基本メカニズムを用いて
シーケンスを学習しているが、各層で学習したシーケンスは異なる方法で使用 ry
。これについて我々が理解していないことはたくさん ry 考えを述べることはできる。
その前に、各層のニューロンが何に接続しているのかを述べると理解の助け ry


上の図は 2 つの新皮質のリージョンとそれらの間の主要な接続関係 ry
このような接続は新皮質の中の互いに関係し合う 2 つのリージョンで一般によく見 ry
。左の箱は、右の（箱の）リージョンよりも低い階層構造にある皮質性のリージョン
を表しているので、フィード・フォワード情報は図の左から右へと流れる。
各リージョンは層に分け ry  2 層と第 3 層は一緒にして第 2/3 層として表されている。



Page 72

色のついた線は異なる層からのニューロンの出力 ry
その層のニューロンから出ている軸索の束である。軸索はすぐに 2 つに分か ry
一つの枝は主にそれと同じ層の中で、リージョン内で水平方向に広がる。
よって各層のすべてのセルは相互によく接続し合っている。
ニューロンと水平方向の接続は図に示されていない。

2 つのフィード・フォワード・パス76がある。オレンジ色で示した直接パスと、
緑色で示した間接パスである。第 4 層は主要なフィード・フォワード入力層で、
両方のフィード・フォワード・パスから入力を受け取る。第 4 層は第 3 層に向かう。

第 3 層は直接フィード・フォワード・パスの始点でもある。
よって、直接フィード・フォワード・パスは第 4 層と第 3 層に限定 ry

いくつかのフィード・フォワード接続は第 4 層を飛ばして直接第 3 層に至る。
そして、上記で述べたように、第 4 層はセンサ入力から遠くにあるリージョンでは
消えて無くなっている。
そこでは直接フォワード・パスは単に第 3 層から次のリージョンの第 3層に繋 ry

2 つめのフィード・フォワード・パス（緑色で示した）は第 5 層から始まる。
第 3層のセルは次のリージョンに至る道筋の中で第 5 層へと接続している。
皮質性の皮から出発した後、第 5 層のセルからの軸索は再び枝分かれする。
1 つの枝は運動の生成に関わる脳内の皮質下部77へと向かう。
これらの軸索は運動指令（下方向の矢印で示した）であると信じられている。
他の枝は脳内の視床78と呼ばれる門として働く部分へと向かう。
視床は次のリージョンに情報を通したり止めたりする。

最後に、黄色で示した主要フィードバック・パスが第 6 層から第 1 層に向かっている。
第 2, 3, 5 層のセルは先端樹状突起（図に示されていない）を経由して第 1 層に向か ry
。第 6 層は第 5 層から入力を受け取る。

この説明は層から層への接続に関して知られていることを限定的に概説 ry 。 ry

異なる層が何をするのかに関する仮説

我々は第 3, 4, 5 層がすべてフィード・フォワード層でありシーケンスを学習していると ry
。第 4 層は一次シーケンスを学習する。第 3 層は可変長シーケンスを学習

76 pathway。通り道。
77 sub-cortical area。大脳皮質の下の神経中枢。
78 thalamus。ししょう。



Page 73

する。第 5 層はタイミングを含む可変長シーケンスを学習する。 ry

第4層
HTM 大脳皮質性学習アルゴリズムを用いて一次シーケンスを学習するのは容易である。
もしカラム中のセルが互いに抑制するように強制しなかったとしたら、
つまりカラム中のセルが以前の入力の文脈を区別しなかったとしたら、一次学習が起こる。
新皮質では、同じカラム内のセルを抑制する効果を取り除くことで成されるだろう。
我々のコンピュータモデルである HTM 大脳皮質性学習アルゴリズムでは、
単にカラムごとに1 つのセルを割り当てることで同様の結果を生む。

一次シーケンスは入力の空間的変形79を表す不変表現80を作る上で必要 ry
例えば視覚では、x-y 変換、縮尺、回転はすべて空間的変形である。
移動する物体について、一次の記憶を持つ HTM リージョンを訓練すると、
異なる空間的パターンが同等であることを学習する。
結果の HTM セルは「複雑型細胞」と呼ばれるもののように振舞う。
その HTM セルはある範囲の空間的変形に対してアクティブな状態（予測状態）を保つ。

Numenta では、視覚についてこのメカニズム ry 実験を行い、
いくつかの空間的不変性が各レベルで達成 ry 。

第 4 層で一次シーケンスを学習していることは、第 4 層で複雑型細胞が見られることや、
なぜ新皮質の高階層のリージョンで第 4 層が消えて無くなるのかということと符合している。
階層構造を上がるにつれて、その時点での表現はすでに不変のものになっているため
それ以上空間的不変性を学習することはできなくなる。

第3層
第 3 層は第 2 章で述べた HTM 大脳皮質性学習アルゴリズムに最も近い。
それは可変シーケンスを学習し、予測を行い、その予測は入力よりも安定している。
第 3 層は常に階層構造の次のリージョンに向かい、
そのため階層構造の中で時間的安定性がより増加するようになる。
可変シーケンス記憶は「方位選択性複雑型細胞」81と呼ばれるニューロンに形成され、
それは第 3 層で最初に観察された。
方位選択性複雑型細胞は例えば左に動いている線と右に動いている線など、
時間的文脈による識別をする。

79 spatial transformation
80 invariant representation
81 directionally-tuned complex cell



Page 74

第5層
最後のフィード・フォワード層は第 5 層である。
我々は第 5 層が第 3 層と似ているが 3 つの違いがあると提唱する。
第一の違いは第 5 層が時間の概念を付加 ry
。第 3 層は次に「何」が起こるかを予測するが、それが「いつ」起こるかを教えてくれない。
しかしながら、話し言葉を理解するときには音の間の相対的なタイミングが重要であるように、
多くのことでタイミングが必要となる。別の例として運動動作がある。
筋肉の活性化のタイミングを揃えることは本質的である。
我々は、第 5 層のニューロンが期待した時刻の後にだけ次の状態を予測すると提唱する。
この仮説を裏付ける生物学上の詳細がいくつかある。一つは第 5 層が新皮質の運動出力層 ry
。いま一つは第 5 層が視床の一部から発して第 1 層から来る入力（図に示されていない）
を受け取るということである。我々はまた、この情報こそが時間をコード化したものであり、
視床を経由して第 1 層に入力される多くのセル（図に示されていない）
にこの情報が分散されると提唱する。

第 3 層と第 5 層の間の第二の違いは第 3 層が可能な限り未来を予測して
時間的安定性をもたらすことが望ましいということである。
第 2 章で述べた HTM 大脳皮質性学習アルゴリズムはこれを行う。
対照的に、第 5 層については次の項目（ある特定の時点の）を予測することしか求めていない。
我々はこの違いをモデル化していないが、
遷移が常に時間を伴って保存されるならそれは自然に起こる。

第 3 層と第 5 層の間の第三の違いは図から見て取れる。
第 5 層の出力は常に皮質下部の運動中枢に向かい、
そのフィード・フォワード・パスは視床の門を通る。
第 5 層の出力は、あるときは次のリージョンへと通過し、またあるときは止められる。
我々（及び他の人）はこの門の働きが潜在的注意82に関係すると提唱する
（潜在的注意は運動行動を伴わずに貴方が入力に注目することである）。

まとめると、第 5 層は特定のタイミング、注意、運動行動を結びつける。
これらが互いにどのように関わりあうかについては多くの謎 ry 。
ry ポイントは、HTM 大脳皮質性学習アルゴリズムのバリエーションが
特定のタイミングを容易に組み入れることができ、別々の皮質の層を結合することができる ry

第2層と第6層
第 6 層は下位のリージョンへフィードバックする軸索の起点である。
第 2 層についてはほとんど知られていない。 ry  2 層が第 3 層と比べて
ユニークな点があるかどうかですら、しばしば議論 ry
。我々はいまのところこの質問に関してこれ以上言えることはほとんどないが、
他のすべての層と同様に第 2 層と第 6 層はたくさんの水平方向の接続パターンを持ち、
カラム単位で反応する特徴があること

82 covert attention



Page 75

だけは指摘することができる。よって我々はこれらもまた、
HTM 大脳皮質性学習アルゴリズムの一形態を実行していると提唱する。

HTM リージョンは新皮質の何に相当するか？
我々は 2 種類の HTM 大脳皮質性学習アルゴリズムを実装した。
一方は可変長記憶のためにカラムごとに複数のセルを持たせるもので、
他方は一次記憶のためにカラムごとに単一のセルを持たせるものである。
我々はこの 2 種類が新皮質の第 3 層と第 4 層に相当 ry
。これら 2 種類を単一の HTM リージョンに結合することを我々はまだ試みていない。

HTM 大脳皮質性学習アルゴリズム（カラムごとに複数のセルを持つ）が
新皮質の第 3 層に最も近いものの、我々のモデルは脳にもない柔軟性を持っている。
よって我々は新皮質のどの層にも相当しない複合型のセルを持つ層を創ることができる。
例えば、我々のモデルでは樹状突起セグメント上でシナプスが形成される順序が分かる。
我々はこの情報を使って、将来起こることのすべてをより一般的に予測した上で
次に何が起こるかを予測 ry 。我々は多分、同様にしてタイミング特有のことを追加 ry
。従って単一の層の HTM リージョンに第 3 層と第 5 層の機能を結合したものを作 ry

まとめ

HTM 大脳皮質性学習アルゴリズムは我々が新皮質の神経器官の基本構成要素
と信じているものを具現化するものである。それは水平接続されたニューロンの層が
どのようにして疎分散表現のシーケンスを学習するのかを示している。
HTM 大脳皮質性学習アルゴリズムの各バリエーションが、
互いに関連するが異なる目的を持つ、新皮質の異なる層で使われる。

我々は新皮質リージョンへのフィード・フォワード入力は、第 4 層であれ第 3 層であれ、
主要樹状突起に主に入力されると提唱する。それは抑制セルの働きにより、
入力の疎分散表現を作成する。
我々はまた、第 2, 3, 4, 5, 6 層のセルがこの疎分散表現を共有していると提唱する。
このことは、それらの層をまたがるカラム中のすべてのセルが
同じフィード・フォワード入力に反応するように強制することによって達成される。

ry 第 4 層のセルが、もしそれが存在するなら、HTM 大脳皮質性学習アルゴリズムを用いて
一次の時間的遷移を学習する ry 。これは空間的遷移に対して不変の表現を構成する。
第 3 層のセルは HTM 大脳皮質性学習アルゴリズムを用いて可変長の時間的遷移を学習し、
皮質の階層を上っていって安定した表現を構成する。
第 5 層のセルはタイミングを伴う可変長の遷移を学習する。
第 2 層と第 6 層については特に提唱



Page 76

するものはない。しかしながら、これらの層でよく見られる水平接続を考えると、
何らかの形でシーケンス記憶を学習 ry



Page 77

用語の説明

ノート： ここでの定義はこの文書で使われている用語の意味であり、
一般的な意味とは異なるものもある。
説明文中で大文字で示されたもの83は、この用語説明で説明されていることを示す。

アクティブ状態（Active State）
フィード・フォワード（Feed-Forward）入力によってセル（Cells）がアクティブになった状態
ボトムアップ（Bottom-Up）84
フィード・フォワード（Feed-Forward）と同義語
セル（Cells）
HTMにおいて、ニューロン（Neuron）に対応するもの。
セル（Cells）はHTMのリージョンにおいてカラムを構成する。
同時発生アクティビティ（Coincident Activity）
同時に2個又はそれ以上のセルがアクティブになること
カラム（Column）
１個又はそれ以上のセルのグループで、HTMリージョンの中で1単位として機能するもの。
カラム中のセルは、同じフィード・フォワード入力の異なる文脈を表現する。
樹状突起セグメント（DendriteSegment）
シナプスが集約した単位で、セルやカラムに結び付けられる。
HTMには二つの異なるタイプの樹状突起セグメントがある。
一つは、あるセルの横方向の接続に結び付けられる。
樹状突起セグメントのアクティブなシナプスの数がしきい値を超えると、
結び付けられたセルが予測状態になる。
もう一方は、あるカラムのフィード・フォワード接続に結び付けられる。
あるカラムのアクティブなシナプスの数がしきい値を超えると、
フィード・フォワードによるアクティブ状態になる。

83 用語説明では、例えば Cell の場合「セル（Cell）」のように示した。
84 原文では Bottom-Up と Feed-Forward の 2 種類の用語が現れるが、
同様の意味であるとされているため、常に「フィード・フォワード」と訳した。



Page 78

望ましい密度
（Desired Density）
リージョン（ Region ）へのフィード・フォワード（Feed-Forward）入力によって
アクティブになるカラム（Column）の望ましいパーセンテージ。
このパーセンテージは、フィード・フォワード入力のファンアウト85に依存して変化
する半径86内にのみ適用される。パーセンテージは個別の入力に応じて変化するものなので
ここでは「望ましい」と呼んでいる。
フィード・フォワード
（Feed-Forward）
階層構造（Hierarchy）の中で、入力が低いレベル（Level）から高いレベル（Level）に向かって
移動すること（しばしば、Bottom-Upと呼ぶ）
フィードバック
（Feedback）
階層構造（Hierarchy）の中で、高いレベル（Level）から低いレベル（Level）に向かって移動
すること（しばしば、Top-Downと呼ぶ）
一次予測
（First Order Prediction）
過去の入力には無関係に、現在の入力だけに依存して予測すること。
可変長予測（Variable Order Prediction）参照。
HTM
（Hierarchical Temporal Memory）
新皮質の構造的・アルゴリズム的機能のいくつかを模写する技術
階層構造
（Hierarchy）
要素間の接続がフィード・フォワード（Feed-Forward）ないし
フィードバック（Feedback）によってユニークに識別されるネットワーク 
HTM大脳皮質性学習アルゴリズム
（HTM Cortical Learning Algorithms）
空間プーリング（Spatial Pooling）、時間プーリング（Temporal Pooling）、
学習と忘却を行う関数一式。HTMリージョン（HTM Region）を構成する。
またの名をHTM学習アルゴリズム（HTM Learning Algorithms）と言う。
HTMネットワーク
（HTM Network）
HTMリージョン（HTM Region）の階層構造（Hierarchy）
HTMリージョン
（HTM Region）
HTMにおいて、記憶と予測（Prediction）を行う主要構成要素。
HTMリージョン（HTM Region）は、カラムの中に配置された高度に相互接続された層
からなる。現状のHTMリージョン（HTM Region）は一層のセルからなるが、
新皮質では（そして完璧なHTMでは）リージョンは複数のセルの層からなる。
階層構造の中の位置という文脈 ry 、リージョンはレベルと呼ばれることがある。

85 fan-out。広がり具合
86 radius



Page 79

推論
（Inference）
空間的ないし時間的入力パターンが、以前に学習したパターンと似ていると認識すること
抑制半径
（InhibitionRadius）
カラム（Column）の周囲の領域で、その範囲でアクティブなカラムが抑制をする範囲を定義する
横方向の接続
（Lateral Connections）
同じリージョン内でのセル（Cells）間の接続関係
レベル
（Level）
階層構造（Hierarchy）の中の HTMリージョン（HTM Region）
ニューロン
（Neuron）
脳内で情報処理を行うセル（Cells）。
本書では、特に生物学的な意味でセルを示すときにニューロンという用語を用い、
単にセルと表記したときはHTMの計算単位を意味する。
永続値
（Permanence）
シナプス候補（Potential Synapse）の接続状態を表すスカラー値。
永続値がしきい値を下回るときシナプスは形成されていないことを表す。
ry しきい値を超えていたら、そのシナプスは有効である。
HTMリージョン（HTM Region）の学習は
シナプス候補（Potential Synapse）の永続値を変更することで達成される。
シナプス候補
（Potential Synapse）
ある樹状突起セグメント（Dendrite Segment）でシナプスを形成する可能性があるセル（Cells）
の部分集合。ある時刻においては、シナプス候補の一部分だけが、有効なシナプスとなる。
有効なシナプスは永続値に基づいて決まる。
予測
（Prediction）
フィード・フォワード（Feed-Forward）入力によって、
セル（Cells）が近い将来アクティブになるであろうということを、
（予測状態の）アクティブ化によって示すこと。
HTMリージョン（HTM Region）はしばしば、将来起こりうる入力を同時に多数予測する。
受容野
（Receptive	Field）
カラム（Column）ないしセル（Cells）が接続されている入力の集合。
HTMリージョン（HTM Region）への入力がビットの2D配列で構成されているとき、
受容野は入力空間内のある半径範囲で表現することができる。
センサー
（Sensor）
HTMネットワーク（HTM Network）への入力源
疎分散表現
（Sparse Distributed Representation）
多くのビットで構成され、そのうちのわずかなパーセンテージだけがアクティブであり、
単一のビットだけでは意味を表現するには不十分であるような表現。



Page 80

空間プーリング
（Spatial Pooling）
入力に対して疎分散表現を計算する処理。空間プーリングの一つの特徴は、
オーバラップする入力パターンを同じ疎分散表現に対応付けられることである。
サブサンプリング
（Sub-Sampling）
大きなパターンのうちのほんのわずかなアクティブビットをマッチングするだけで、
大きな分散パターンを認識すること
シナプス
（Synapse）
学習によって形成されるセル（Cells）間の接続
時間プーリング
（Temporal Pooling）
入力パターンのシーケンスの表現を計算する処理。結果の表現は入力よりも安定したものになる。
トップダウン
（Top-Down）
フィードバック（Feedback）の同義語
可変長予測
（Variable Order Prediction）
それが依存する直前の文脈の量が変化するような予測。一次予測（First Order Prediction）参照。
直前の文脈を維持管理するためのメモリを必要に応じて割り当てるため、「可変長」と呼ばれる。
そのため可変長予測を実装するメモリシステムは、
指数関数的に増大するメモリを必要とすることなく、文脈を時間的に後戻りすることができる。
